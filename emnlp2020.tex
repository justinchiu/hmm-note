%
% File emnlp2020.tex
%
%% Based on the style files for ACL 2020, which were
%% Based on the style files for ACL 2018, NAACL 2018/19, which were
%% Based on the style files for ACL-2015, with some improvements
%%  taken from the NAACL-2016 style
%% Based on the style files for ACL-2014, which were, in turn,
%% based on ACL-2013, ACL-2012, ACL-2011, ACL-2010, ACL-IJCNLP-2009,
%% EACL-2009, IJCNLP-2008...
%% Based on the style files for EACL 2006 by 
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{emnlp2020}
\usepackage{times}
\usepackage{latexsym}
\renewcommand{\UrlFont}{\ttfamily\small}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

\usepackage{mystyle}

%\aclfinalcopy % Uncomment this line for the final submission
%\def\aclpaperid{***} %  Enter the acl Paper ID here

%\setlength\titlebox{5cm}
% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.

\newcommand\BibTeX{B\textsc{ib}\TeX}

\title{Instructions for EMNLP 2020 Proceedings}

\author{First Author \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  \texttt{email@domain} \\\And
  Second Author \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  \texttt{email@domain} \\}

\date{}

\begin{document}
\maketitle
\begin{abstract}
todo
\end{abstract}

\section{Introduction}

todo


\section{Background}
We focus on language modeling,
where we learn a distribution over sequences of tokens
at the word level $\bx = \set{x_0, \ldots, x_T}$, with each token $x_t$
an element of the finite vocabulary $\mcX$.

\subsection{Hidden Markov Models}
Hidden Markov Models (HMMs) specify a joint distribution over 
the observed words $\bx$ and discrete latent states $\bz = \set{z_0, \ldots, z_T}$,
each $z_t$ from finite set $\mcZ$,
with the following generative process:
For every time step $t \in \set{0,\ldots,T}$, choose a state given the previous state
$z_t \mid z_{t-1}$ from the transition distribution $p(z_t \mid z_{t-1})$,
where the first state $z_0$ is chosen from the starting distribution $p(z_0)$.
Then choose a word given the current state $x_t \mid z_t$ from the emission distribution $p(x_t \mid z_t)$.
This yields the joint distribution
\begin{equation}
p(\bx, \bz)
= p(z_0)p(x_0 \mid z_0)\prod_{t=1}^T p(x_t\mid z_t)p(z_t \mid z_{t-1})
\end{equation}

%By virtue of modeling the joint distribution over $\bx_{0:T},\bz_{0:T}$,
%HMMs maintain uncertainty over both observed words the unobserved latent states.
%Contrast this with another class of language models, recurrent neural networks (RNNs),
%which only maintain uncertainty over the observed $\bx_{0:T}$.

%At every timestep $t$, the generative process bottlenecks all information from the past
%through a single discrete state $z_t$ from a finite set.
%This contrasts 

The distributions are parameterized as follows
\begin{equation}
\label{param}
\begin{aligned}
p(z_0) &\propto e^{\phi_{z_0}}\\
p(z_t \mid z_{t-1}) &\propto e^{\phi_{z_tz_{t-1}}}\\
p(x_t \mid z_t) &\propto e^{\phi_{x_tz_t}}
\end{aligned}
\end{equation}
where each $\phi_{z_0},\phi_{z_tz_{t-1}},\phi_{x_tz_t} \in \mathbb{R}$
may be parameterized by a scalar or a neural network.

\section{Model}
For language modeling, we are interested in the distribution over the observed words
obtained by marginalizing over the latent states $p(\bx) = \sum_{\bz}p(\bx,\bz)$.
This sum can be computed in time $O(T|\mcZ|^2)$ via dynamic programming,
which becomes prohibitive if the number of latent states $|\mcZ|$ is large.
However, in order to increase the capacity of HMMs, we must consider large $\mcZ$.
We outline several techniques to manage the computational complexity of marginalization in HMMs.  

\subsection{Sparse Emission HMMs}
We introduce a sparse emission constraint that allows us to
efficiently perform conditional inference in large state spaces.
%(TODO: clarify difference in related work. should we also run this as a baseline?
%intuition: not enough parameter sharing if each state only emits a single word.
%can we prove that you need more states if the emission distribution is overconstrained?)
Inspired by Cloned HMMs \citep{dedieu2019learning},
we constrain each word $x$ to be emit only by $z \in \mcC_x \subset \mcZ$:
\begin{equation}
\label{eqn:sparse_emission}
p(x \mid z) \propto 1(z \in \mcC_x)e^{\phi_{zx}}
\end{equation}
This allows us to perform marginalization as follows
\begin{equation}
\label{eqn:sparse_marginalization}
\begin{aligned}
p(\bx) &= \sum_{z_0} p(z_0)p(x_0 \mid z_0)
    \sum_{z_1}p(z_1\mid z_0)p(x_1 \mid z_1)\\
    &\qquad\cdots
    \sum_{z_T}p(z_T \mid z_{T-1})p(x_T \mid z_T)\\
&= \sum_{z_0 \in \mcC_{x_0}} p(z_0)p(x_0 \mid z_0)
    \sum_{z_1 \in \mcC_{x_1}} p(z_1\mid z_0)p(x_1 \mid z_1)\\
    &\qquad\cdots
    \sum_{z_T \in \mcC_{x_T}} p(z_T \mid z_{T-1})p(x_T \mid z_T)
\end{aligned}
\end{equation}
which takes $O(T \max_x(|\mcC_x|)^2)$ computation.
We choose sets $\mcC_x$ such that $\forall x, |\mcC_x| = k$
due to convenience of implementation.

We experiment with two constraint sets, both of which are specified 
as hyperparameters and remain fixed for a given model.
The first constraint set is obtained by sampling subsets of
states for each word i.i.d. from a uniform distribution over subsets.
The second constraint set uses Brown Clusters \citep{brown1992}
to assign all words in a given Brown cluster the same subset of states.
This constraint set is advantageous, as it admits further optimizations
that we discuss below.


\subsection{State Dropout}
We introduce a variant of dropout called state dropout that operates on the
start and transition distributions.
The goal of state dropout is to encourage usage of the full state space in HMMs
as well as to reduce the complexity of marginalization in a manner
identical to the sparse emission constraints.

State dropout samples a mask over states
$\mathbf{b}_\mcC \in \set{0,1}^{|\mcC|}$ for each partition $\mcC \subset \mcZ$,
such that each sampled $|\mathbf{b}_\mcC| = n$.
%(TODO: Mention multivariate hypergeometric?)
We only apply this if the $\mcC_x$ are disjoint partitions,
as with the Brown constraint set.

Let $\mathbf{b}$ be the concatenation of the $\mathbf{b}_\mcC$ for all partitions $\mcC$,
such that $b_z = 1$ if $b_{\mcC z} = 1$, where $z \in \mcC$.
The start and transition distributions with dropout are given by
\begin{equation}
\label{eqn:state_dropout}
\begin{aligned}
p(z_0) &\propto b_{z_0}e^{\phi_{z_0}}\\
p(z_t \mid z_{t-1}) &\propto b_{z_t}b_{z_{t-1}}e^{\phi_{z_tz_{t-1}}}
\end{aligned}
\end{equation}

Marginalizing over $\bz$ with state dropout requires $O(Tn^2)$ computation,
where $n$ is the number of nonzero elements of $\mathbf{b}_\mcC, \forall \mcC$.
We utilize state dropout on top of the Brown emission sparsity constraint,
subsampling states of size $n$ for each partition during training.

We also consider unstructured dropout with rate $p$
which samples elements of masks $b_{z_0}, b_{z_tz_{t-1}}\overset{iid}{\sim} \Bern(p)$
for all $z_0, z_t, z_{t-1}\in\mcZ$, yielding
\begin{equation}
\label{eqn:unstructured_dropout}
\begin{aligned}
p(z_0) &\propto b_{z_0}e^{\phi_{z_0}}\\
p(z_t \mid z_{t-1}) &\propto b_{z_tz_{t-1}}e^{\phi_{z_tz_{t-1}}}.
\end{aligned}
\end{equation}
Unstructured dropout results in sparsity patterns that are
more difficult to take advantage of than state dropout,
since there is variance in the number of nonzero elements
in a mask.
(Fix wording)

We find that state dropout has similar regularization effect as unstructured dropout,
but additionally yields computational benefits as described above.

% interaction with brown

\subsection{Learning}
We optimize the evidence of observed sentences $\log p(\bx) = \log \sum_\bz p(\bx,\bz)$
by first marginalizing over latent states $\bz$ then performing gradient ascent.
The emission sparsity constraints we introduce as well as state dropout
allow us to both perform marginalization
and compute the gradient of the evidence efficiently.

\section{Experiments}
We evaluate the HMM on two language modeling datasets.

\subsection{Language Modeling}
\paragraph{Datasets}
The two datasets we used are the \texttt{Penn Treebank} \citep{ptb}
and \texttt{wikitext2} \citep{wikitext}.
\texttt{Penn Treebank} contains 929k tokens in the training corpus,
with a vocabulary size of 10k.
\texttt{Wikitext2} contains 2M tokens in the training corpus,
with a vocabulary size of 33k.
Both datasets contain inter-sentence dependencies,
due to the use of un-shuffled documents.

\paragraph{Implementation}
We train two-layer LSTM recurrent neural networks with 256 or 512 units,
as well as two-layer feed-forward neural networks with 256 or 512 units.
The HMMs we train follow the sparsity constraints outlined in the previous
section with a dropout rate of 0.5,
and we vary the total number of states as well as states per word.
We optimize all models with AdamW \citep{adamw}.
See Appendix \ref{sec:hyperparams} for the hyperparameters for all models.

\subsection{Results}
% Only PTB results for now.
We report validation (TODO: will include test) perplexities 
for \texttt{Penn Treebank} in Table \ref{tbl:ppl-ptb} and for 
\texttt{wikitext2} in Table \ref{tbl:ppl-wikitext2}.
The HMMs in all cases outperform the feedforward models (FF),
but underperform the recurrent LSTMs.

In the following sections, we ablate and analyze the HMMs on \texttt{Penn Treebank}.

% transition?

\begin{table}[!t]
\centering
\caption{\label{tbl:ppl-ptb}
Perplexities on the \texttt{Penn Treebank} dataset.
The FF model is a 256-dim 2-layer feedforward neural network
with a window of 4 previous tokens with 0.3 dropout.
The LSTM is a 256-dim 2-layer recurrent neural network with 0.3 dropout.
The HMM is a 32k-state HMM with 0.5 state dropout.
}
\begin{tabular}{llll}
\toprule
Model & Num Params & Valid PPL & Test PPL\\
\midrule
FF    &            & 160         & -       \\
LSTM  &            & 90          & -       \\
HMM   &            & 124         & -       \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[!t]
\centering
\caption{\label{tbl:ppl-wikitext2}
Perplexities on the \texttt{wikitext2} dataset.
The FF model is a 256-dim 2-layer feedforward neural network
with a window of 4 previous tokens with 0.3 dropout.
The LSTM is a 256-dim 2-layer recurrent neural network with 0.3 dropout.
The HMM is a 32k-state HMM with 0.5 state dropout.
}
\begin{tabular}{llll}
\toprule
Model & Num params & Valid PPL & Test PPL\\
\midrule
FF    &            & 209       & -       \\
LSTM  &            & 125       & -       \\
HMM   &            & 167       & -       \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Sparse emission constraint ablation}
We ablate the emission sparsity constraints in Table \ref{tbl:ppl-assn-ablation},
and find that the Brown emission constraints outperforms the uniform emission constraints
in all model sizes.

One explanation for the relative benefit of Brown emission constraints over uniform
is due to the nature of Brown clusters.
The goal of Brown clustering is to place two words in the same cluster
if they are used in the same context.
In Table \#, we observe that the entropy of the emission distribution
for models with uniform emission
constraints is lower than models with brown constraints.
This implies that since the uniform models are less likely to place words that appear in
similar contexts together the burden of modeling ambiguity is pushed onto
the transition distribution.

% explain why this hurts the model? or is it already clear
% compare entropies to exact 1k HMM.
% probably find that brown is closer to exact. does this hold true for the 4 brown cluster HMM?

% Include separate table with emission and transition entropies
% am i computing those correctly?

%Analysis
%\begin{itemize}
%\item Hypothesis: Brown Cluster assigns words that are emit in similar contexts.
%Uniform assignment with too few states per word will not group words together
%resulting in needing more states.
%\item Evidence: check entropy of emission distribution / Uniform should be lower entropy.
%\item Conclusion: Suboptimal assignments lead to needing more states
%to achieve the same performance.
%\end{itemize}

\begin{table}[!t]
\centering
\caption{\label{tbl:ppl-assn-ablation}
The perplexities for the different emission sparsity constraints
in a 1024 state HMM as well as larger HMMs for which exact
inference without sparsity is too expensive.
The quantities $|\mcZ|$ and $k$ are the number of hidden
states and the number of states per word respectively.
The HMMs with 1024 states do not have any dropout,
while the 8k and 16k state HMMs have unstructured dropout.
}

\begin{tabular}{llll}
\toprule
Constraint & $|\mcZ|$ & $k$    & Valid PPL\\
\midrule
Uniform    & 1k       & 32     & -\\
Brown      & 1k       & 32     & -\\
None       & 1k       & 1k     & -\\
\midrule 
Uniform    & 8k       & 128    & 150\\
Brown      & 8k       & 128    & 142\\
Uniform    & 16k      & 128    & 146\\
Brown      & 16k      & 128    & 134\\
\bottomrule
\end{tabular}
\end{table}



\paragraph{Dropout analysis}
Graph 1: Show dropout helps but variants don't affect performance too much,
and encourages better state usage (vs no dropout).
Justify state dropout as the one with simplest implementation.
\begin{itemize}
\item Brown 16k 128 + unstructured dropout: ppl and state usage
\item Brown 16k 128 + state dropout
\item Brown 16k 128 + no dropout
\end{itemize}

Analysis 1: Discuss what happens with just state dropout, without partitioning.
(Too much variance in gradient estimator, would require variance reduction.
Get numbers or just handwaive?
Consider exact inference (in constrained model) this work.
)

\paragraph{Number of Brown clusters}
We next examine the sensitivity of HMMs to the number of Brown clusters
in Table \ref{tbl:ppl-spw-ablation}.
We find that models at with 16k or 32k total states
are not sensitive to the number of Brown clusters
in the range where marginalization is computationally feasible.
This contrasts with the observation that the number of Brown clusters
influenced performance at 1k total states.
% FILL IN HERE, reconcile differences

\begin{table}[!t]
\centering
\caption{\label{tbl:ppl-spw-ablation}
The perplexities for HMMs with 16k states
and different numbers of Brown clusters
for constraining the emission distribution of the HMMs.
$|\mcZ|$ is the total number of hidden states.
All models have 0.5 state dropout.
}

\begin{tabular}{llll}
\toprule
$|\mcZ|$ & Num clusters & Valid PPL\\
\midrule
16k      & 32           & 136\\
16k      & 64           & 137\\
16k      & 128          & 133\\
16k      & 256          & 137\\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Weight tying ablation}
Analysis 1: Discuss performance using different tying methods
(found this to not affect performance) and computational savings
in terms of number of parameters.
Discuss relative parameter inefficiency compared to LSTM / FF.

\paragraph{State size ablation}
Graph 1: Show performance improves as we increase the total number of states
\begin{itemize}
\item Brown HMM + state dropout, 16k, 128 clusters
\item Brown HMM + state dropout, 32k, 128 clusters
\item Brown HMM + state dropout, 65k, 128 clusters
\end{itemize}

Analysis 1: How does state usage change as clusters remain constant
but the number of states (and states per word) increases?

\paragraph{Parameterization ablation}
Analysis 1: Is there a performance difference between neural / scalar parameterization,
and is it consistent across state sizes?

Discussion 1: Memory savings when working with state dropout to not instantiate the
full matrices during training, which makes working with a neural parameterization
beneficial.

\subsection{Error analysis}
\paragraph{}
%\paragraph{}

\section{Discussion}

%\subsection{POS Induction}
%\subsection{Word sense induction}

\section*{Acknowledgments}
TBD

\bibliographystyle{acl_natbib}
\bibliography{anthology,emnlp2020}

\appendix

\section{Hyperparameters}
\label{sec:hyperparams}
LSTM
\begin{itemize}
\item 2 layers
\end{itemize}

\section{Parameter estimation}
We maximize the evidence of the observed sentences
$\log p(\bx) = \log \sum_{\bz} p(\bx, \bz)$
via gradient ascent.

\subsection{Gradient of the evidence}
Let $\psi_0(z_0, z_1) = \log p(x_0, z_0)$ and
$\psi_t(z_{t}, z_{t+1}) = \log p(x_{t+1}, z_{t+1} \mid z_{t})$ for $t \in \set{1, \ldots, T-1}$.
Additionally, let $\oplus$ and $\otimes$ be addition and multiplication in the log semiring.
After conditioning on the observed $\bx$, we can express the evidence as the following:
\begin{equation}
\label{eqn:evidence}
A_x = \log p(\bx) = \bigoplus_{\bz}\bigotimes_{t=0}^{T-1} \psi_t(z_t, z_{t+1})
\end{equation}
where $A_\bx$ is the clamped log partition function (after observing $\bx_{0:T}$).

We show that the gradient of the log partition function with respect to the $\psi_t(z_t, z_{t+1})$
is the first moment (a general result for the cumulant generating function
of exponential family distributions). Given the value of the derivative
$\frac{\partial A_\bx}{\partial \psi_t(z_t, z_{t+1})}$,
we can then apply the chain rule to compute the total derivative with respect to
downstream parameters. For example, the gradient with respect to the transition matrix
of the HMM is given by
$$\frac{\partial A_\bx}{\partial \theta_{ij}}
= \sum_t \frac{\partial A_\bx}{\partial \psi_t(i,j)}
\frac{\partial \psi_t(i,j)}{\partial \theta_{ij}}$$

Recall that the derivative of logaddexp ($\oplus$ in the log semiring) is
\begin{equation}
\label{eqn:lse_derivative}
\begin{aligned}
\frac{\partial}{\partial x} x \oplus y
&= \frac{\partial}{\partial x} \log e^x + e^ y\\
&= \frac{e^x}{e^x + e^y}\\
&= \exp(x - (x \oplus y))
\end{aligned}
\end{equation}
while the derivative of addition ($\otimes$ in the log semiring) is
\begin{equation}
\label{eqn:plus_derivative}
\frac{\partial}{\partial x} x \otimes y = 1
\end{equation}

%(TODO name variables to make this readable in 2col format)
The derivative of the clamped log partition function $A_\bx$ is given by
\begin{align*}
&\frac{\partial A_\bx}{\partial \psi_t(i,j)}\\
&= \frac{\partial}{\partial \psi_t(i,j)} \bigoplus_{\bz}
    \bigotimes_{t'} \psi_{t'}(z_{t'}, z_{t'+1})\\
&= \frac{\partial}{\partial \psi_t(i,j)} \Bigg(
        \Bigg(\bigoplus_{\bz:z_t = i, z_{t+1} = j}
        \bigotimes_{t'} \psi_{t'}(z_{t'}, z_{t'+1})\Bigg)\\
& \qquad \bigoplus
        \Bigg(\bigoplus_{\bz:z_t \ne i, z_{t+1} \ne j}
        \bigotimes_{t'} \psi_{t'}(z_{t'}, z_{t'+1})\Bigg)
    \Bigg)\\
&= \exp\Bigg(\Bigg(\bigoplus_{\bz:z_t = i, z_{t+1} = j} 
    \bigotimes_{t'} \psi_{t'}(z_{t'}, z_{t'+1})\Bigg)\\
&\qquad - \Bigg(\Bigg(\bigoplus_{\bz:z_t = i, z_{t+1} = j}
        \bigotimes_{t'} \psi_{t'}(z_{t'}, z_{t'+1})\Bigg)\\
    &\qquad \qquad \bigoplus
        \Bigg(\bigoplus_{\bz:z_t \ne i, z_{t+1} \ne j}
        \bigotimes_{t'} \psi_{t'}(z_{t'}, z_{t'+1})\Bigg)
    \Bigg)\Bigg)\\
&= \exp\left(\left(\bigoplus_{\bz:z_t = i, z_{t+1} = j} 
    \bigotimes_{t'} \psi_{t'}(z_{t'}, z_{t'+1})\right)
    - A_\bx\right)
\end{align*} 
which is the value at $i$ and $j$ of the
edge marginal for $z_t, z_{t+1}$ obtained via the forward-backward algorithm.
The second equality is a result of the associativity of $\oplus$;
the third equality is a result of Eqn. \ref{eqn:lse_derivative};
and the fourth equality from Eqn. \ref{eqn:plus_derivative}.


\section{Supplemental Material}
\label{sec:supplemental}
\end{document}
