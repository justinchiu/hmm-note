%
% File emnlp2020.tex
%
%% Based on the style files for ACL 2020, which were
%% Based on the style files for ACL 2018, NAACL 2018/19, which were
%% Based on the style files for ACL-2015, with some improvements
%%  taken from the NAACL-2016 style
%% Based on the style files for ACL-2014, which were, in turn,
%% based on ACL-2013, ACL-2012, ACL-2011, ACL-2010, ACL-IJCNLP-2009,
%% EACL-2009, IJCNLP-2008...
%% Based on the style files for EACL 2006 by 
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{emnlp2020}
\usepackage{times}
\usepackage{latexsym}
\renewcommand{\UrlFont}{\ttfamily\small}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

\usepackage{mystyle}
\usepackage{subcaption}

%% ADD BACK!!!!!!!!!!!!!!!!!
%\aclfinalcopy % Uncomment this line for the final submission
%\def\aclpaperid{***} %  Enter the acl Paper ID here

%\setlength\titlebox{5cm}
% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.

\newcommand\BibTeX{B\textsc{ib}\TeX}

\title{Instructions for EMNLP 2020 Proceedings}

\author{First Author \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  \texttt{email@domain} \\\And
  Second Author \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  \texttt{email@domain} \\}

\date{}

\begin{document}
\maketitle
\begin{abstract}
todo
\end{abstract}

\section{Introduction}


% Hidden Markov models are classic models that have been abandoned.
Hidden Markov models (HMMs) are a fundamental model in machine learning,
providing the simplest latent-variable model for sequential data.
HMMs are core to time-series problems in bioinformatics, reinforcement learning, and,
of course, natural language processing.
HMMs have remained a core building block,
because they cleanly separate out the facet of discrete model memory from
the generation of observations.
Historically they have been used extensively in NLP for tasks such as
sequence modeling \citep{rabiner1990tut},
alignment \citep{vogel1996hmm},
and less extensively language modeling \citep{kuhn1994hmmlm,huang2011thesis}. 

%In recent years, most models in 
%For the task of language modeling,
%HMMs were explored in work by \citet{kuhn1994hmmlm,huang2011thesis}
%but were not revisited until recently \citep{krakovna2016hmm,dedieu2019learning}.

In recent years, most state-of-the-art systems in NLP have moved away utilizing explicit hidden states.
(Something about sequence modleing)
particularly for structured models.
In the area of language models modeled the observed words using n-gram models, 
feedforward neural networks \citep{bengio2003nlm},
and eventually recurrent neural networks \citep{mikolov2010rnn,zaremba2014lstm,merity2017awdlstm}
or transformers \citep{radford2019language}.
% what do we gain by modeling dynamics
This progress has led to the common wisdom that latent-variable models are not
competitive with fully observed models.


In this work, we revisit the question of developing HMMs for natural language processing,
in particular looking at the tasks of language modeling and sequence tagging.
We approach this question by taking direct lessons from the success of deep neural models for NLP tasks.
In particular we make three observations:
(1) The right parameterization is critically important for representation learning, e.g. a feedforward model \citet{bengio2003nlm}
can have the same probabilistic structure as an n-gram model while performing significantly better;
(2) overparameterization seems to be critical for finding better optima,
e.g. large LSTMs \citet{zaremba2014lstm} show marked improvements in performance;
(3) regularization choices are necessary to handle reconcile (different) parameterizations,
e.g. experiments by \citet{merity2017awdlstm} outline a variety of training choices.

These observations motivate a new approach to classical  HMMs.
We develop a neural parameterization for HMMs that extends 
them to comparable size and structure of deep learning models,
while allowing us to lazily instantiate distributions.
We also develop a sparsity constraint that allows us to utilize very large HMMs,
significantly beyond standard efficiency limits.
Finally we incorporate a variant of dropout that both improves accuracy
and reduces the computational overhead by an order of magnitude during training. 
    
Experiments show that on two language modeling datasets and a supervised sequence tagging task.
We find that our HMM extension significantly outperforms past HMMs as well as n-gram models. 
It also performs comparably to neural counterparts with a similar number of parameters
while maintaining uncertainty over the state dynamics.

\section{Related work}

% Prior work has tried neural parameterizations, but with small state spaces.
Prior work has demonstrated the benefits of neural parameterization of structured generative models. 
For HMMs, \citet{tran2016hmm} demonstrated improvements in POS induction with a
neural parameterization of an HMM,
while \citet{kim2019cpcfg} demonstrated improvements in grammar induction with
a probabilistic context free grammar.
Both of these works used latent variables with relatively small state spaces,
as the goal of both was structure induction rather than language modeling itself.
We extend the neural parameterization to much larger state space models.

% Prior work has tried large state spaces with scalar parameterizations.
We also draw inspiration from the experiments with
cloned HMMs by \citet{dedieu2019learning},
who proposed to introduce sparsity constraints in scalar
emission distribution of HMMs in order to make conditional inference
tractable in large state spaces.
\citet{dedieu2019learning} trained a 30k state HMM on character-level language modeling
by constraining every state to emit only a single character type.
This particular constraint is problematic for language modeling at the word level,
where the vocabulary size is much larger.
We build on their work by proposing a sparsity constraint based on
Brown clustering \citep{brown1992} which allows us to extend their
work to vocabularies that are larger than the state space.

% State splitting / refinement
Another approach to scaling to larger state spaces is to initialize
with a small state space then grow the state space via a split-merge process
\citep{petrov2006splitmerge,huang2011thesis}.
In particular, \citet{huang2011thesis} learn an HMM for language modeling
via this process.
Additionally, the cloned HMM \citep{dedieu2019learning} can be seen
as an HMM that starts with a single state per word,
then splits every state into $k$ states at the start
with no subsequent splits or merges.
The application of split-merge is an avenue for future work,
as we focus on fixed-size state spaces for simplicity.

Other investigations into improving the performance of HMMs
involved relaxing independence or modeling assumptions \citep{buys2018hmm}
to resemble a recurrent neural network, or through model combination
with a recurrent neural network \citep{krakovna2016hmm}.
There are also extensions of HMMs, such as factorial HMMs \cite{zoubin1997fhmm,nepal2013fhmm}
and context free grammars \citep{kim2019cpcfg}.
We leave scaling more expressive models to large state spaces for future work,
and focus on scaling the basic HMM.

\begin{figure}[t]
\centering
\begin{tikzpicture}[]
\node[latent] (z0) {$z_0$} ;
\node[latent] (z1) [right=1.25cm of z0] {$z_1$} ;
\node[latent] (z2) [right=1.25cm of z1] {$z_2$} ;
\node[obs]    (x0) [below = 0.75cm of z0] {$x_0$};
\node[obs]    (x1) [below = 0.75cm of z1] {$x_1$};
\node[obs]    (x2) [below = 0.75cm of z2] {$x_2$};

\edge {z0} {x0};
\edge {z1} {x1};
\edge {z2} {x2};
\edge {z0} {z1};
\edge {z1} {z2};
\end{tikzpicture}

\caption{
\label{fig:hmm}
An HMM with tokens $x_t$ and states $z_t$.
}
\end{figure}


\section{Background: HMMs}
We are interested in learning a distribution over observed tokens
$\bx = \langle x_1, \ldots, x_T \rangle$, with each token $x_t$
an element of the finite vocabulary $\mcX$.
Hidden Markov models (HMMs) specify a joint distribution over 
observed tokens $\bx$ and discrete latent states $\bz = \langle z_1, \ldots, z_T \rangle$,
with each $z_t$ from finite set $\mcZ$.
For notational convenience, we define the starting state $z_0=\epsilon$.
The model is defined by the following generative process (shown in figure~\ref{fig:hmm}):
For every time step $t \in \set{0,\ldots,T}$, choose a state given the previous state
$z_t \mid z_{t-1}$ from the transition distribution $p(z_t \mid z_{t-1})$.
Then choose a token given the current state $x_t \mid z_t$
from the emission distribution $p(x_t \mid z_t)$.
This yields the joint distribution
\begin{equation}
p(\bx, \bz; \theta)
= \prod_{t=1}^T p(x_t\mid z_t)p(z_t \mid z_{t-1})
\end{equation}

%By virtue of modeling the joint distribution over $\bx_{0:T},\bz_{0:T}$,
%HMMs maintain uncertainty over both observed words the unobserved latent states.
%Contrast this with another class of language models, recurrent neural networks (RNNs),
%which only maintain uncertainty over the observed $\bx_{0:T}$.

%At every timestep $t$, the generative process bottlenecks all information from the past
%through a single discrete state $z_t$ from a finite set.
%This contrasts 

\noindent The distributions are parameterized as follows
\begin{equation}
\label{param}
\begin{aligned}
p(z_1 | z_0) &\propto e^{\psi_{z_1}}\\
p(z_t \mid z_{t-1}) &\propto e^{\psi_{z_tz_{t-1}}}\\
p(x_t \mid z_t) &\propto e^{\phi_{x_tz_t}}
\end{aligned}
\end{equation}
where each $\psi_{z_0},\psi_{z_tz_{t-1}},\phi_{x_tz_t} \in \mathbb{R}$.
Thus the transition distribution $p(z_t \mid z_{t-1})$ is parameterized by
$\psi \in \mathbb{R}^{|\mcZ|\times|\mcZ|}$,
and the emission distribution $p(x_t \mid z_{t})$ by $\phi \in \mathbb{R}^{|\mcX| \times |\mcZ|}$.

We distinguish two types of parameterizations: \textit{scalar} and \textit{neural}.
A scalar parameterization simply uses $\theta = \set{\phi,\psi}$ to fit one model parameter for
each distributional parameter.
This results in $O(|\mcZ|^2 + |\mcX||\mcZ|)$ model parameters,
since the transition and emission matrices must be explicitly represented.
This parameterization can lead to either over or under parameterization for most NLP tasks.
In contrast, a neural parameterization uses $\theta$ as the parameters of a neural network
that generates $\phi$ and $\psi$ for the distribution.
These may be customized for the task through embedding or other layers,
and separate the parameterization size from the hidden state size. 

In order to fit an HMM to data $\bx$,
we must marginalize over the latent states to obtain the likelihood
$p(\bx) = \sum_{\bz}p(\bx,\bz)$.
This sum can be computed in time $O(T|\mcZ|^2)$ via dynamic programming,
which becomes prohibitive if the number of latent states $|\mcZ|$ is large.
As the dynamic program is differentiable, we can optimize the likelihood 
with gradient ascent.
An alternative method would be to employ expectation maximization,
however there is no closed form solution for the M-step with a neural parameterization.
Gradient ascent is applicable to both the scalar and neural parameterizations.

(Talk about constructing matrices once per batch,
decoupling cost of softmax over vocab and batch size,
constructing log potentials via indexing,
then logarithmic reduction DP. )

\paragraph{Comparison to RNNs}
RNNs also model $p(x)$, but without introducing a latent variable for the state.
Whereas RNNs encode the previous observations with a single distributed representation,
HMMs encode the past as a distribution over a finite set of states.
Given a hidden size of $h$ for the RNN and a cost $v(h)$ for computing
a matrix-vector product, the computational complexity of
computing $p(x)$ for a single sequence is $O(Tv(h))$ due to the use
of matrix-vector products at every timestep.
For an HMM with a state space of size $|\mcZ| = h$ and matrix-vector product cost $v(h)$,
the complexity of computing $p(\bx) = \sum_z p(\bx,\bz)$ on sequential
hardware is also $O(Tv(h))$.
However, on parallel device, we are able to compute $p(\bx)$
for an HMM in time logarithmic in $T$ using the parallel prefix-sum trick. 
Assuming a cost $m(h)$ for matrix multiplication, this gives cost
$O(\log(T)m(h))$ for an HMM on a parallel device.
Quasi-RNNs \citep{bradbury2016qrnn} also have a logarithmic dependency on $T$
by applying the same prefix-sum trick, but the operation applied is 
less expensive than a matrix multiplication.

\section{Method}

HMMs cleanly separate out latent memory from the generative process of language.
This separation makes them an important model for isolating observed properties and performing inference.
However, this separation also means they require a large hidden state space to
capture the underlying properties of language. 

In this section, we consider three methods for greatly expanding the state space of HMMs
without similarly increasing inference complexity or parameterization size. 

\subsection{Emission Sparsity Constraints}
Marginal inference for general HMMs is quadratic in the  state space $|\mcZ|$.
In practice, this limits the size of the state space to the order of 100s,
which prevents models from having enough states to capture the full history.
However, we can limit the complexity in special cases.
In particular, if we know that the probability of emitting a word $x_t$ from a state $z_t$ is 0,
i.e. $p(x_t \mid z_t) = 0$  then we can ignore that state during inference. 

Inspired by cloned HMMs \citep{dedieu2019learning},
we add explicitly constrain the model to ensure that this properties holds at every time step.
Specifically, we ensure that for an observed token $x$
a fixed number of states $z$ have $p(x \mid z) > 0$.
For each word, we call this set $\mcC_x \subset \mcZ$.
This yields the following constrained emission distribution:
\begin{equation}
\label{eqn:sparse_emission}
p(x \mid z) \propto 1(z \in \mcC_x)e^{\phi_{xz}}
\end{equation}
This allows us to perform marginal inference as follows
\begin{equation}
\label{eqn:sparse_marginalization}
\begin{aligned}
&p(\bx)\\
&= \sum_{z_0} p(z_0)p(x_0 \mid z_0)
    \sum_{z_1}p(z_1\mid z_0)p(x_1 \mid z_1)\\
    &\qquad\cdots
    \sum_{z_T}p(z_T \mid z_{T-1})p(x_T \mid z_T)\\
&= \sum_{z_0 \in \mcC_{x_0}} p(z_0)p(x_0 \mid z_0)
    \sum_{z_1 \in \mcC_{x_1}} p(z_1\mid z_0)p(x_1 \mid z_1)\\
    &\qquad\cdots
    \sum_{z_T \in \mcC_{x_T}} p(z_T \mid z_{T-1})p(x_T \mid z_T)
\end{aligned}
\end{equation}
The key step in the second equality was only summing
over the respective $z_t$ with nonzero $p(x_t \mid z_t)$ for each $x_t$.
This allows us to compute $p(\bx)$ with $T$ matrix-vector products,
each matrix of dimension $|\mcC_{x_t-1}| \times |\mcC_{x_{t}}|$.
We choose sets $\mcC_x$ such that $\forall x, |\mcC_x| = k$
yielding a computation complexity of $O(Tk^2)$ rather than $O(T|\mcZ|^2)$.
This gives a massive speedup in practice, for example if $|\mcZ|$ is on the order of
10k and $k$ in the 100s.
Please refer to Fig.~\ref{fig:trellis} for an illustration of this method.
(Describe here)

This method is a generalization of the one introduced in the cloned HMM \citep{dedieu2019learning},
as we constrain the emission distribution such that every token is emit by $k$
states, but a given state may emit more than one token.
The cloned HMM constrains states to emit only a single token,
which prevents its application to problems where the token space is larger than the state space.

\tikzstyle{state}=[shape=circle,draw=blue!50,fill=blue!20]
\tikzstyle{observation}=[shape=rectangle,draw=orange!50,fill=orange!20]
\tikzstyle{lightedge}=[<-,dotted]
\tikzstyle{mainstate}=[state,thick]
\tikzstyle{mainedge}=[<-,thick]
\tikzstyle{transedge}=[<-]


\begin{figure}[t]
\begin{center}
\begin{tikzpicture}[]
% 1st column
\node[latent] (s1_1) at (0,4) {$z_{0,0}$};
\node[latent] (s2_1) at (0,3) {$z_{0,1}$};
\node[latent] (s3_1) at (0,2) {$z_{0,2}$};
\node[latent] (s4_1) at (0,1) {$z_{0,3}$};
\node[obs]    (x0) at (0,5) {$x_0$}
    edge[transedge, bend left=40] (s1_1)
    edge[transedge, bend left=40] (s2_1);
% 2nd column
\node[latent] (s1_2) at (2,4) {$z_{1,0}$};
\node[latent] (s2_2) at (2,3) {$z_{1,1}$};
\node[latent] (s3_2) at (2,2) {$z_{1,2}$}
    edge[transedge] (s1_1)
    edge[transedge] (s2_1);
\node[latent] (s4_2) at (2,1) {$z_{1,3}$}
    edge[transedge] (s1_1)
    edge[transedge] (s2_1);
\node[obs]    (x1)    at (2,5) {$x_1$}
    edge[transedge, bend left=40] (s3_2)
    edge[transedge, bend left=40] (s4_2);
% 3rd column
\node[latent] (s1_3) at (4,4) {$z_{2,0}$}
    edge[transedge] (s3_2)
    edge[transedge] (s4_2);
\node[latent] (s2_3) at (4,3) {$z_{2,1}$}
    edge[transedge] (s3_2)
    edge[transedge] (s4_2);
\node[latent] (s3_3) at (4,2) {$z_{2,2}$};
\node[latent] (s4_3) at (4,1) {$z_{2,3}$};
\node[obs]    (x2)    at (4,5) {$x_2$}
    edge[transedge, bend left=40] (s1_3)
    edge[transedge, bend left=40] (s2_3);
\end{tikzpicture}
\end{center}
\caption{
\label{fig:trellis}
{\color{red} WILL REPLACE WITH TRELLIS}
A depiction of the emission sparsity constraints.
%The partitions the states such that.
%Above is the trellis after conditioning on the observation sequence $\bx$
%with hidden states $\bz$.
%The arrows indicate transitions with nonzero probabilities.
%Rather than considering the full transition matrices at every timestep,
%the emission constraint allows us to use much smaller matrices
%after conditioning on $\bx$.
}
\end{figure}

\subsection{State Dropout}
We introduce a variant of dropout called state dropout that operates on the
emission distribution.
The goal of state dropout is to encourage usage of the full state space in HMMs
as well as to reduce the complexity of marginalization in a manner
that compounds with the sparse emission constraints discussed above.

State dropout prevents a token $x$ from always being emit by a specific state $z$
by zeroing out emission probabilities during training.
Recall that the HMM has distributional parameters
$\phi \in \mathbb{R}^{|\mcX|\times|\mcZ|}$ for the emission matrix,
and the emission sparsity constraints associate each token $x$
with a set of states $\mcC_x$ that are the only states which 
emit $x$ with nonzero probability.
State dropout samples a mask over states $\mathbf{b}_x \in \set{0,1}^{|\mcC_x|}$
such that $|\mathbf{b}_x| = n$ for each $x$.
Given the mask, we then computes the following emission distribution:
\begin{equation}
\label{eqn:state_dropout}
p(x \mid z) \propto b_{xz}1(z \in \mcC_x)e^{\phi_{xz}}
\end{equation}

Performing marginal inference with state dropout requires $O(Tn^2)$ computation,
where $n$ is the number of nonzero elements of $\mathbf{b}_\mcC, \forall \mcC$.

\subsection{Parameterization}
We parameterize the transition and emission distributions using a residual network
with LayerNorm.
Let $\ba,\bb\in\mathbb{R}^h$, where $h$ is the hidden dimension of our neural network
and the size of our token and state embeddings.
We use the following neural network for $i \in \set{\phi,\psi}$
\begin{equation}
\begin{aligned}
f_i(A) &= g(\textrm{ReLU}(W_{i1}A))\\
g_i(B) &= \textrm{LayerNorm}(\textrm{ReLU}(W_{i2}B) + B)
\end{aligned}
\end{equation}
to define the distributional parameters,
where $A,B\in\mathbb{R}^{h \times |\mcZ|}$ and $W_{i1},W_{i2} \in \mathbb{R}^{h \times h}$.
Let $V_x \in \mathbb{R}^{h \times |\mcX|}$ be the token embeddings
and $U_{\textrm{previous}},U_{\textrm{current}},U_\textrm{preterminal}
\in \mathbb{R}^{h \times |\mcZ|}$ the state embeddings.
The distributional parameters are given by
\begin{equation}
\begin{aligned}
\phi &= V_x^\top f_\phi(U_\textrm{preterminal})\\
\psi &= U_\textrm{current}^\top f_\psi(U_\textrm{previous})
\end{aligned}
\end{equation}
where $\phi \in \mathbb{R}^{|X|\times|Z|}$ and $\psi \in \mathbb{R}^{|Z|\times|Z|}$.

Compared to fully representing $\phi,\psi$ with a scalar parameterization
which would require $O(|\mcZ|^2 + |\mcX||\mcZ|)$,
this neural parameterization takes $O(h^2 + h|\mcZ| + h|\mcX|)$ parameters
where $h$ is in the 100s and $|\mcZ|,|\mcX|$ are on the order of 10k.

\section{Experiments}
In this section we provide further details on the application
of the methods described in the previous section
to language modeling and sequence tagging.

\subsection{Language modeling}
In word-level language modeling the tokens $\bx$ correspond to the words
$\bw = \langle w_1,\ldots,w_T \rangle$ in a sentence.
We thus learn a model over words and states
$p(\bw,\bz) = \prod_t p(w_t\mid z_t)p(z_t \mid z_{t-1})$.

We use Brown clusters \citep{brown1992} to create the emission constraints.
Brown clusters are obtained by assigning every token type in $\mcX$ a state in a HMM,
then continually merging states until a desired number of states is reached.
The merge at each iteration attempts to merge the two states that results in best likelihood.

To apply the Brown clusters as an emission constraint,
each Brown cluster cluster is assigned a disjoint set of $k$ states
such that the state space is partitioned.
Each state corresponding to a Brown cluster is then allowed to emit only
the words in that Brown cluster.
This constraint corresponds to learning a refinement of the Brown clusters
by splitting each cluster into $k$ states.

Since the emission constraints using Brown clusters partitions the state space,
we share state dropout masks for all word types within a Brown cluster.

(Parameterization of embeddings, factoring cluster + word to reduce params)

\subsection{Sequence Tagging}
We extend the language modeling HMM to part of speech (POS) tagging,
where the tokens $\bx$ are now the product of words $\bw$ as well as tags
$\by = \langle y_1, \ldots, y_T \rangle$.

At every timestep, our model emits both a word $w_t$ and tag $y_t$
independently given the states $z_t$.
This yields the following joint distribution
\begin{equation}
\begin{aligned}
p(\bw, \by, \bz)
&= \prod_t p(w_t \mid z_t)p(y_t \mid z_t)p(z_t\mid z_{t-1})\\
\end{aligned}
\end{equation}

We use the same emission constraints and dropout strategy as language modeling,
but applied only to the word emission distribution $p(w_t \mid z_t)$.
The tag emission distribution is unconstrained.

(Parameterization, CharCNN)

\subsection{Datasets}
\paragraph{Language modeling}
We evaluate on the \texttt{Penn Treebank} \citep{ptb}
and \texttt{wikitext2} \citep{wikitext} datasets.
\texttt{Penn Treebank} contains 929k tokens in the training corpus,
with a vocabulary size of 10k.
We use the preprocessing from \citet{mikolov-2011},
which lowercases all words and substitutes words outside of the vocabulary
with unks. 
\texttt{Wikitext2} contains 2M tokens in the training corpus,
with a vocabulary size of 33k.
Casing is preserved, and all words outside the vocab are unked.
Both datasets contain inter-sentence dependencies,
due to the use of documents consisting of more than a single sentence.

\paragraph{Part of speech tagging}
We use the Wall Street Journal portion of the \texttt{Penn Treebank}
for evaluating POS tagging.
We map all numbers to 0, and perform no further preprocessing.
We train on sections 0-18, validate on sections 19-21, and test on 22-24
following \citet{ma2016crf}.


\begin{comment}
\paragraph{Implementation}
We train two-layer LSTM recurrent neural networks with 256 units,
as well as two-layer feed-forward neural networks with 256 units.
The HMMs we train follow the sparsity constraints outlined in the previous
section with a dropout rate of 0.5,
and we vary the total number of states as well as states per word.
We optimize all models with AdamW \citep{adamw}.

We experimented with a couple batching strategies:
On \texttt{Penn Treebank},
the first strategy discarded the inter-sentence dependencies and shuffled all sentences,
and the second treated the corpus as a single flat document without shuffling.
On \texttt{Wikitext2}, we either shuffled at the document level or treated the corpus as a
single document.
Prior work on both corpuses treated the corpora as single documents.

See Appendix \ref{sec:hyperparams} for the hyperparameters for all models.
\end{comment}

\section{Results}
% Only PTB results for now.
We report perplexities 
for \texttt{Penn Treebank} in Table \ref{tbl:ppl-ptb} and for 
\texttt{wikitext2} in Table \ref{tbl:ppl-wikitext2}.

The HMMs in all cases outperform the feedforward models (FF),
but underperform the recurrent LSTMs.
Since the HMMs require parameters linear in the number of hidden states,
we find that the performance of the HMMs scales poorly compared to the other models
which only require parameters that scale linearly with the vocbaulary size.
Although representing and summing over the hidden states
allows us to explicitly capture uncertainty in the hidden state,
it proves to be a limiting factor in terms of performance.

In the remainder of this section we ablate and analyze the HMMs on \texttt{Penn Treebank}.

% transition?

\begin{table*}[!t]
\centering
\caption{\label{tbl:ppl-ptb}
Perplexities on the \texttt{Penn Treebank} dataset.
The FF model is a 256-dim 2-layer feedforward neural network
with a window of 4 previous tokens with 0.3 dropout.
The LSTM is a 256-dim 2-layer recurrent neural network with 0.3 dropout.
The HMM is a 64k-state HMM with 0.5 state dropout.
}
\begin{tabular}{lllll}
\toprule
Dataset & Model & Num Params & Valid PPL & Test PPL\\
\midrule
\texttt{Penn Treebank}
& FF    & 2.8M       & 164         & -       \\
& LSTM  & 3.6M       & 93.55       & 88.83   \\
& HMM   & 7.7M       & 125.02      & 115.82  \\
\midrule
\texttt{Wikitext-2}
& FF    &            & 209       & -       \\
& LSTM  &            & 125       & -       \\
& HMM   &            & 167       & -       \\
\bottomrule
\end{tabular}
\end{table*}


\begin{comment}

\begin{table}[!t]
\centering
\caption{\label{tbl:ppl-wikitext2}
Perplexities on the \texttt{wikitext2} dataset.
The FF model is a 256-dim 2-layer feedforward neural network
with a window of 4 previous tokens with 0.3 dropout.
The LSTM is a 256-dim 2-layer recurrent neural network with 0.3 dropout.
The HMM is a 32k-state HMM with 0.5 state dropout.
}
\begin{tabular}{llll}
\toprule
Model & Num params & Valid PPL & Test PPL\\
\midrule
FF    &            & 209       & -       \\
LSTM  &            & 125       & -       \\
HMM   &            & 167       & -       \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Sparse emission constraint ablation}
We ablate the emission sparsity constraints in Table \ref{tbl:ppl-assn-ablation},
and find that the Brown emission constraints outperforms the uniform emission constraints
in all model sizes.

One explanation for the relative benefit of Brown emission constraints over uniform
is due to the effect of Brown clusters.
The goal of Brown clustering is to place two words in the same cluster
if they are used in the same context.
In Table \ref{tbl:entropy}, we observe that the entropy of the emission distribution
for models with uniform emission
constraints is lower than models with brown constraints,
and the entropy of the transition distributions is higher.
This implies that the burden of modeling ambiguity is pushed onto
the transition distribution,
since the uniform models are less likely to place words that appear in
similar contexts together.

% explain why this hurts the model? or is it already clear
% compare entropies to exact 1k HMM.
% probably find that brown is closer to exact. does this hold true for the 4 brown cluster HMM?

% Include separate table with emission and transition entropies
% am i computing those correctly?

%Analysis
%\begin{itemize}
%\item Hypothesis: Brown Cluster assigns words that are emit in similar contexts.
%Uniform assignment with too few states per word will not group words together
%resulting in needing more states.
%\item Evidence: check entropy of emission distribution / Uniform should be lower entropy.
%\item Conclusion: Suboptimal assignments lead to needing more states
%to achieve the same performance.
%\end{itemize}

\begin{table}[!t]
\centering
\caption{\label{tbl:ppl-assn-ablation}
The perplexities for the different emission sparsity constraints
in a 1024 state HMM as well as larger HMMs for which exact
inference without sparsity is too expensive.
The quantities $|\mcZ|$ and $k$ are the number of hidden
states and the number of states per word respectively.
The HMMs with 1024 states do not have any dropout,
while the 8k and 16k state HMMs have unstructured dropout
at a rate of 0.5.
}

\begin{tabular}{llll}
\toprule
Constraint & $|\mcZ|$ & $k$    & Valid PPL\\
\midrule
Uniform    & 1k       & 32     & -\\
Brown      & 1k       & 32     & -\\
None       & 1k       & 1k     & -\\
\midrule 
Uniform    & 8k       & 128    & 150*\\
Brown      & 8k       & 128    & 142*\\
Uniform    & 16k      & 128    & 146*\\
Brown      & 16k      & 128    & 134*\\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[!t]
\centering
\caption{\label{tbl:entropy}
The average entropies of the the emission and
transition distributions for HMMs
with uniform and Brown cluster emission constraints.
All models have $k=128$ states per word
and use unstructured dropout with a rate of $p=0.5$.
}

\begin{tabular}{llll}
\toprule
Constraint & $|\mcZ|$ & H(emit)& H(transition)\\
\midrule 
Uniform    & 8k       & -      & - \\
Brown      & 8k       & -      & - \\
Uniform    & 16k      & -      & - \\
Brown      & 16k      & -      & - \\
\bottomrule
\end{tabular}
\end{table}


\paragraph{Dropout analysis}
The effect of the different dropout strategies and rates is 
shown in Table~\ref{tbl:dropout}.
We found that state dropout outperformed unstructured dropout overall.
This in addition to the computational benefits afforded by state dropout
led us to prefer this strategy.

Unstructured dropout was sensitive to batching strategies.
We observed a larger gap between training and validation perplexities
with the unshuffled batching strategy,
whereas unstructured dropout appeared to be robust to the batching strategy.

Analysis 1: Discuss what happens with just state dropout, without partitioning.
(Too much variance in gradient estimator, would require variance reduction.
Get numbers or just handwaive?
Consider exact inference (in constrained model) this work.
)

\begin{table}[!t]
\centering
\caption{\label{tbl:dropout}
State occupancies for the dropout strategies and rates.
All models were HMMs with Brown cluster emission constraints,
16k total states, and 128 states per word (and therefore 128 Brown clusters).
}

\begin{tabular}{llll}
\toprule
Dropout type & $p$  & Valid PPL & Occupancy\\
\midrule 
Unstructured & 0.25 & -         & - \\
Unstructured & 0.5  & -         & - \\
Unstructured & 0.75 & -         & - \\
State        & 0.25 & -         & - \\
State        & 0.5  & -         & - \\
State        & 0.75 & -         & - \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Number of Brown clusters}
We next examine the sensitivity of HMMs to the number of Brown clusters
in Table \ref{tbl:ppl-spw-ablation}.
We find that models at with 16k or 32k total states
are not sensitive to the number of Brown clusters
in the range where marginalization is computationally feasible.
This contrasts with the observation that the number of Brown clusters
influenced performance at 1k total states.
% FILL IN HERE, reconcile differences

\begin{table}[!t]
\centering
\caption{\label{tbl:ppl-spw-ablation}
The perplexities for HMMs with 16k states
and different numbers of Brown clusters
for constraining the emission distribution of the HMMs.
$|\mcZ|$ is the total number of hidden states.
All models have 0.5 state dropout.
}

\begin{tabular}{llll}
\toprule
$|\mcZ|$ & Num clusters & Valid PPL\\
\midrule
16k      & 32           & 136\\
16k      & 64           & 137\\
16k      & 128          & 133\\
16k      & 256          & 137\\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Weight tying ablation}
Analysis 1: Discuss performance using different tying methods
(found this to not affect performance) and computational savings
in terms of number of parameters.
Discuss relative parameter inefficiency compared to LSTM / FF.

\paragraph{State size ablation}
We find that as we increase the number of total states
while keeping the Brown clusters fixed,
performance improves up until we have 65k states.

Analysis 1: How does state usage change as clusters remain constant
but the number of states (and states per word) increases?

\begin{table}[!t]
\centering
\caption{\label{tbl:ppl-states-ablation}
The perplexities for HMMs with 128 Brown clusters
for constraining the emission distribution of the HMMs.
$|\mcZ|$ is the total number of hidden states.
All models have 0.5 state dropout.
}

\begin{tabular}{llll}
\toprule
$|\mcZ|$ & Num clusters & Valid PPL\\
\midrule
16k      & 128          & 133\\
32k      & 256          & 126\\
65k      & 512          & 124\\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Parameterization ablation}
Analysis 1: Is there a performance difference between neural / scalar parameterization,
and is it consistent across state sizes?

Discussion 1: Memory savings when working with state dropout to not instantiate the
full matrices during training, which makes working with a neural parameterization
beneficial.
\end{comment}

\subsection{Error analysis}
\paragraph{}
%\paragraph{}

\section{Discussion}

%\subsection{POS Induction}
%\subsection{Word sense induction}

\section*{Acknowledgments}
TBD

\bibliographystyle{acl_natbib}
\bibliography{anthology,emnlp2020}

\appendix

\section{Hyperparameters}
\label{sec:hyperparams}
LSTM
\begin{itemize}
\item 2 layers
\end{itemize}

\begin{comment}
\section{Parameter estimation}
We maximize the evidence of the observed sentences
$\log p(\bx) = \log \sum_{\bz} p(\bx, \bz)$
via gradient ascent.

\subsection{Gradient of the evidence}
Let $\psi_0(z_0, z_1) = \log p(x_0, z_0)$ and
$\psi_t(z_{t}, z_{t+1}) = \log p(x_{t+1}, z_{t+1} \mid z_{t})$ for $t \in \set{1, \ldots, T-1}$.
Additionally, let $\oplus$ and $\otimes$ be addition and multiplication in the log semiring.
After conditioning on the observed $\bx$, we can express the evidence as the following:
\begin{equation}
\label{eqn:evidence}
A_x = \log p(\bx) = \bigoplus_{\bz}\bigotimes_{t=0}^{T-1} \psi_t(z_t, z_{t+1})
\end{equation}
where $A_\bx$ is the clamped log partition function (after observing $\bx_{0:T}$).

We show that the gradient of the log partition function with respect to the $\psi_t(z_t, z_{t+1})$
is the first moment (a general result for the cumulant generating function
of exponential family distributions). Given the value of the derivative
$\frac{\partial A_\bx}{\partial \psi_t(z_t, z_{t+1})}$,
we can then apply the chain rule to compute the total derivative with respect to
downstream parameters. For example, the gradient with respect to the transition matrix
of the HMM is given by
$$\frac{\partial A_\bx}{\partial \theta_{ij}}
= \sum_t \frac{\partial A_\bx}{\partial \psi_t(i,j)}
\frac{\partial \psi_t(i,j)}{\partial \theta_{ij}}$$

Recall that the derivative of logaddexp ($\oplus$ in the log semiring) is
\begin{equation}
\label{eqn:lse_derivative}
\begin{aligned}
\frac{\partial}{\partial x} x \oplus y
&= \frac{\partial}{\partial x} \log e^x + e^ y\\
&= \frac{e^x}{e^x + e^y}\\
&= \exp(x - (x \oplus y))
\end{aligned}
\end{equation}
while the derivative of addition ($\otimes$ in the log semiring) is
\begin{equation}
\label{eqn:plus_derivative}
\frac{\partial}{\partial x} x \otimes y = 1
\end{equation}

%(TODO name variables to make this readable in 2col format)
The derivative of the clamped log partition function $A_\bx$ is given by
\begin{align*}
&\frac{\partial A_\bx}{\partial \psi_t(i,j)}\\
&= \frac{\partial}{\partial \psi_t(i,j)} \bigoplus_{\bz}
    \bigotimes_{t'} \psi_{t'}(z_{t'}, z_{t'+1})\\
&= \frac{\partial}{\partial \psi_t(i,j)} \Bigg(
        \Bigg(\bigoplus_{\bz:z_t = i, z_{t+1} = j}
        \bigotimes_{t'} \psi_{t'}(z_{t'}, z_{t'+1})\Bigg)\\
& \qquad \bigoplus
        \Bigg(\bigoplus_{\bz:z_t \ne i, z_{t+1} \ne j}
        \bigotimes_{t'} \psi_{t'}(z_{t'}, z_{t'+1})\Bigg)
    \Bigg)\\
&= \exp\Bigg(\Bigg(\bigoplus_{\bz:z_t = i, z_{t+1} = j} 
    \bigotimes_{t'} \psi_{t'}(z_{t'}, z_{t'+1})\Bigg)\\
&\qquad - \Bigg(\Bigg(\bigoplus_{\bz:z_t = i, z_{t+1} = j}
        \bigotimes_{t'} \psi_{t'}(z_{t'}, z_{t'+1})\Bigg)\\
    &\qquad \qquad \bigoplus
        \Bigg(\bigoplus_{\bz:z_t \ne i, z_{t+1} \ne j}
        \bigotimes_{t'} \psi_{t'}(z_{t'}, z_{t'+1})\Bigg)
    \Bigg)\Bigg)\\
&= \exp\left(\left(\bigoplus_{\bz:z_t = i, z_{t+1} = j} 
    \bigotimes_{t'} \psi_{t'}(z_{t'}, z_{t'+1})\right)
    - A_\bx\right)
\end{align*} 
which is the value at $i$ and $j$ of the
edge marginal for $z_t, z_{t+1}$ obtained via the forward-backward algorithm.
The second equality is a result of the associativity of $\oplus$;
the third equality is a result of Eqn. \ref{eqn:lse_derivative};
and the fourth equality from Eqn. \ref{eqn:plus_derivative}.
\end{comment}


\section{Supplemental Material}
\label{sec:supplemental}
\end{document}
