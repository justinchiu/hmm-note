%
% File emnlp2020.tex
%
%% Based on the style files for ACL 2020, which were
%% Based on the style files for ACL 2018, NAACL 2018/19, which were
%% Based on the style files for ACL-2015, with some improvements
%%  taken from the NAACL-2016 style
%% Based on the style files for ACL-2014, which were, in turn,
%% based on ACL-2013, ACL-2012, ACL-2011, ACL-2010, ACL-IJCNLP-2009,
%% EACL-2009, IJCNLP-2008...
%% Based on the style files for EACL 2006 by 
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{emnlp2020}
\usepackage{times}
\usepackage{latexsym}
\renewcommand{\UrlFont}{\ttfamily\small}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

\usepackage{mystyle}

%\aclfinalcopy % Uncomment this line for the final submission
%\def\aclpaperid{***} %  Enter the acl Paper ID here

%\setlength\titlebox{5cm}
% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.

\newcommand\BibTeX{B\textsc{ib}\TeX}

\title{Instructions for EMNLP 2020 Proceedings}

\author{First Author \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  \texttt{email@domain} \\\And
  Second Author \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  \texttt{email@domain} \\}

\date{}

\begin{document}
\maketitle
\begin{abstract}
todo
\end{abstract}

\section{Introduction}

todo

\section{Related work}
%\paragraph{Observed language models}
Practitioners have used neural models
to improve language modeling performance by utilizing
architectures that capture long-range dependencies,
moving from feedforward architectures \citep{bengio2003nlm}
to recurrent neural networks \citep{mikolov2010rnn,zaremba2014lstm,merity2017awdlstm}
as well as transformer-based architectures \citep{radford2019language}.

%\paragraph{Latent lms}
A recent line of work has investigated the inclusion of latent variables 
in sequence models \citep{bowman2015vae,kim2019urnng},
with the aim of learning interpretable latent spaces.
However, the inclusion of latent variables often complicates training
and the performance of latent variable models has not been able to match
that of fully observed models.

To make matters worse,
exact inference itself is intractable in many latent variable sequence models,
especially those with continuous state spaces. 
We therefore limit our focus to models that permit exact inference,
and in particular models with discrete latent variables.

% Neural parameterizations of classical LVMs
Recent work has demonstrated improvements in tractable discrete latent variable models,
such as hidden Markov models (HMMs) and probabilistic context free grammars,
through neural parameterizations of
conditional distributions \citep{tran2016hmm,kim2019cpcfg}.
In the case of context free grammars,
\citet{kim2019cpcfg} found that incorporating 
a compound mixture led to performance improvements,
which allowed for potentially infinitely many states
while sacrificing exact inference.

%\paragraph{Scaling HMMs}
An alternative method for increasing the size of the state space which
also retains tractability of exact inference was presented in
experiments by \citet{dedieu2019learning},
which introduced a sparsity constraint in the emission distribution of an HMM.
The constraint they applied allowed each state to emit only a single element,
which allowed them to train a 30k state HMM on character-level language modeling.
This particular constraint is problematic for language modeling at the word level,
as the size of the vocabulary for a given corpus may be greater than 30k,
leaving only a single state per word.

In this work, we build off of \citet{dedieu2019learning} 
to examine whether HMMs with tens of thousands of states
can serve as effective language models.
We introduce a new sparsity constraint and a variant of dropout
for HMMs that allows us to efficiently train with a large number of states.
With these techniques, we demonstrate that the performance of HMMs scales with the
size of the state space, but does not reach the performance of recurrent neural networks
but exceeds that of n-gram models.
{\color{red}TODO: LM PPL, POS induction}

% Insert somewhere
Other investigations into improving the performance of HMMs
involve relaxing independence or modeling assumptions \citep{buys2018hmm}
to resemble a recurrent neural network, or through model combination
with a recurrent neural network \citep{krakovna2016hmm}.

%\paragraph{Models with Sparsity}
%\paragraph{Interpretability}

\section{Background}
We focus on language modeling,
where we learn a distribution over sequences of tokens
at the word level $\bx = \set{x_0, \ldots, x_T}$, with each token $x_t$
an element of the finite vocabulary $\mcX$.

\subsection{Hidden Markov Models}
Hidden Markov Models (HMMs) specify a joint distribution over 
the observed words $\bx$ and discrete latent states $\bz = \set{z_0, \ldots, z_T}$,
with each $z_t$ from finite set $\mcZ$.
The model is defined by the following generative process:
For every time step $t \in \set{0,\ldots,T}$, choose a state given the previous state
$z_t \mid z_{t-1}$ from the transition distribution $p(z_t \mid z_{t-1})$,
where the first state $z_0$ is chosen from the starting distribution $p(z_0)$.
Then choose a word given the current state $x_t \mid z_t$ from the emission distribution $p(x_t \mid z_t)$.
This yields the joint distribution
\begin{equation}
p(\bx, \bz)
= p(z_0)p(x_0 \mid z_0)\prod_{t=1}^T p(x_t\mid z_t)p(z_t \mid z_{t-1})
\end{equation}

%By virtue of modeling the joint distribution over $\bx_{0:T},\bz_{0:T}$,
%HMMs maintain uncertainty over both observed words the unobserved latent states.
%Contrast this with another class of language models, recurrent neural networks (RNNs),
%which only maintain uncertainty over the observed $\bx_{0:T}$.

%At every timestep $t$, the generative process bottlenecks all information from the past
%through a single discrete state $z_t$ from a finite set.
%This contrasts 

The distributions are parameterized as follows
\begin{equation}
\label{param}
\begin{aligned}
p(z_0) &\propto e^{\phi_{z_0}}\\
p(z_t \mid z_{t-1}) &\propto e^{\phi_{z_tz_{t-1}}}\\
p(x_t \mid z_t) &\propto e^{\phi_{x_tz_t}}
\end{aligned}
\end{equation}
where each $\phi_{z_0},\phi_{z_tz_{t-1}},\phi_{x_tz_t} \in \mathbb{R}$
may be parameterized by a scalar or a neural network.

A scalar parameterization would result in $O(|\mcZ|^2 + |\mcZ||\mcX|)$ parameters,
since the transition and emission matrices must be explicitly represented.
A compact neural parameterization may take as few as $O(H(|\mcZ| + |\mcX|))$ parameters,
where $H$ is the dimension of the state and word embeddings.
Such a parameterization represents the states and words with embeddings,
and implicitly parameterizes the transition and emission distributions
by modeling those matrices as a function of the embeddings.
We use a residual network as in \citet{kim2019cpcfg} to parameterize conditional distributions.

\section{Model}
For sequence modeling with an HMM,
we are interested in the distribution over the observed
obtained by marginalizing over the latent states $p(\bx) = \sum_{\bz}p(\bx,\bz)$.
This sum can be computed in time $O(T|\mcZ|^2)$ via dynamic programming,
which becomes prohibitive if the number of latent states $|\mcZ|$ is large.
However, in order to increase the capacity of HMMs, we must consider large $\mcZ$.
We outline two techniques to manage the computational complexity of marginalization in HMMs.  

\subsection{Sparse Emission HMMs}
We introduce a sparse emission constraint that allows us to
efficiently perform conditional inference in large state spaces.
%(TODO: clarify difference in related work. should we also run this as a baseline?
%intuition: not enough parameter sharing if each state only emits a single word.
%can we prove that you need more states if the emission distribution is overconstrained?)
Inspired by Cloned HMMs \citep{dedieu2019learning},
we constrain each observation $x$ to be emit only by $z \in \mcC_x \subset \mcZ$:
\begin{equation}
\label{eqn:sparse_emission}
p(x \mid z) \propto 1(z \in \mcC_x)e^{\phi_{zx}}
\end{equation}
This allows us to perform marginalization as follows
\begin{equation}
\label{eqn:sparse_marginalization}
\begin{aligned}
&p(\bx)\\
&= \sum_{z_0} p(z_0)p(x_0 \mid z_0)
    \sum_{z_1}p(z_1\mid z_0)p(x_1 \mid z_1)\\
    &\qquad\cdots
    \sum_{z_T}p(z_T \mid z_{T-1})p(x_T \mid z_T)\\
&= \sum_{z_0 \in \mcC_{x_0}} p(z_0)p(x_0 \mid z_0)
    \sum_{z_1 \in \mcC_{x_1}} p(z_1\mid z_0)p(x_1 \mid z_1)\\
    &\qquad\cdots
    \sum_{z_T \in \mcC_{x_T}} p(z_T \mid z_{T-1})p(x_T \mid z_T)
\end{aligned}
\end{equation}
which takes $O(T \max_x(|\mcC_x|)^2)$ computation.
We choose sets $\mcC_x$ such that $\forall x, |\mcC_x| = k$
due to convenience of implementation.

We experiment with two constraint sets, both of which are specified 
as hyperparameters and remain fixed for a given model.
The first constraint set is obtained by sampling subsets of
states for each element of $\mcX$ i.i.d. from a uniform distribution over subsets.
The second constraint set uses Brown Clusters \citep{brown1992}
to assign all words in a given Brown cluster the same subset of states.
This constraint set is advantageous, as it admits further optimizations
that we discuss below.

\subsection{State Dropout}
We introduce a variant of dropout called state dropout that operates on the
start and transition distributions.
The goal of state dropout is to encourage usage of the full state space in HMMs
as well as to reduce the complexity of marginalization in a manner
identical to the sparse emission constraints.

State dropout samples a mask over states
$\mathbf{b}_\mcC \in \set{0,1}^{|\mcC|}$ for each partition $\mcC \subset \mcZ$,
such that each sampled $|\mathbf{b}_\mcC| = n$.
%(TODO: Mention multivariate hypergeometric?)
We only apply this if the $\mcC_x$ are disjoint partitions,
as with the Brown constraint set.

Let $\mathbf{b}$ be the concatenation of the $\mathbf{b}_\mcC$ for all partitions $\mcC$,
such that $b_z = 1$ if $b_{\mcC z} = 1$, where $z \in \mcC$.
The start and transition distributions with dropout are given by
\begin{equation}
\label{eqn:state_dropout}
\begin{aligned}
p(z_0) &\propto b_{z_0}e^{\phi_{z_0}}\\
p(z_t \mid z_{t-1}) &\propto b_{z_t}b_{z_{t-1}}e^{\phi_{z_tz_{t-1}}}
\end{aligned}
\end{equation}

Marginalizing over $\bz$ with state dropout requires $O(Tn^2)$ computation,
where $n$ is the number of nonzero elements of $\mathbf{b}_\mcC, \forall \mcC$.
We utilize state dropout on top of the Brown emission sparsity constraint,
subsampling states of size $n$ for each partition during training.

We also consider unstructured dropout with rate $p$,
which samples elements of masks $b_{z_0}, b_{z_tz_{t-1}}\overset{iid}{\sim} \Bern(1-p)$
for all $z_0, z_t, z_{t-1}\in\mcZ$, yielding
\begin{equation}
\label{eqn:unstructured_dropout}
\begin{aligned}
p(z_0) &\propto b_{z_0}e^{\phi_{z_0}}\\
p(z_t \mid z_{t-1}) &\propto b_{z_tz_{t-1}}e^{\phi_{z_tz_{t-1}}}.
\end{aligned}
\end{equation}
Unstructured dropout results in sparsity patterns that are
more difficult to take advantage of than state dropout,
since there is variance in the number of nonzero elements
in a mask.
(Fix wording)

We find that models with state dropout
generalize better than models with unstructured dropout,
and additionally have computational benefits as described above.

\subsection{Learning}
We optimize the evidence of observed sentences $\log p(\bx) = \log \sum_\bz p(\bx,\bz)$
by first marginalizing over latent states $\bz$ then performing gradient ascent.
The emission sparsity constraints we introduce as well as state dropout
allow us to both perform marginalization
and compute the gradient of the evidence efficiently.

\section{Experiments}
We evaluate the HMM on two language modeling datasets.

\subsection{Language Modeling}
\paragraph{Datasets}
The two datasets we used are the \texttt{Penn Treebank} \citep{ptb}
and \texttt{wikitext2} \citep{wikitext}.
\texttt{Penn Treebank} contains 929k tokens in the training corpus,
with a vocabulary size of 10k.
We use the preprocessing from \citet{mikolov-2011},
which lowercases all words and substitutes words outside of the vocabulary
with unks. 
\texttt{Wikitext2} contains 2M tokens in the training corpus,
with a vocabulary size of 33k.
Casing is preserved, and all words outside the vocab are unked.
Both datasets contain inter-sentence dependencies,
due to the use of documents consisting of more than a single sentence.

\paragraph{Implementation}
We train two-layer LSTM recurrent neural networks with 256 or 512 units,
as well as two-layer feed-forward neural networks with 256 or 512 units.
The HMMs we train follow the sparsity constraints outlined in the previous
section with a dropout rate of 0.5,
and we vary the total number of states as well as states per word.
We optimize all models with AdamW \citep{adamw}.

We experimented with a couple batching strategies:
On \texttt{Penn Treebank},
the first strategy discarded the inter-sentence dependencies and shuffled all sentences,
and the second treated the corpus as a single flat document without shuffling.
On \texttt{Wikitext2}, we either shuffled at the document level or treated the corpus as a
single document.
Prior work on both corpuses treated the corpora as single documents.

See Appendix \ref{sec:hyperparams} for the hyperparameters for all models.

\subsection{Results}
% Only PTB results for now.
We report perplexities 
for \texttt{Penn Treebank} in Table \ref{tbl:ppl-ptb} and for 
\texttt{wikitext2} in Table \ref{tbl:ppl-wikitext2}.

The HMMs in all cases outperform the feedforward models (FF),
but underperform the recurrent LSTMs.
Since the HMMs require parameters linear in the number of hidden states,
we find that the performance of the HMMs scales poorly compared to the other models
which only require parameters that scale linearly with the vocbaulary size.
Although representing and summing over the hidden states
allows us to explicitly capture uncertainty in the hidden state,
it proves to be a limiting factor in terms of performance.

In the remainder of this section we ablate and analyze the HMMs on \texttt{Penn Treebank}.

% transition?

\begin{table}[!t]
\centering
\caption{\label{tbl:ppl-ptb}
Perplexities on the \texttt{Penn Treebank} dataset.
The FF model is a 256-dim 2-layer feedforward neural network
with a window of 4 previous tokens with 0.3 dropout.
The LSTM is a 256-dim 2-layer recurrent neural network with 0.3 dropout.
The HMM is a 64k-state HMM with 0.5 state dropout.
}
\begin{tabular}{llll}
\toprule
Model & Num Params & Valid PPL & Test PPL\\
\midrule
FF    & 2.8M       & 164         & -       \\
LSTM  & 3.6M       & 97          & -       \\
HMM   & 19.8M      & 122         & -       \\
HMM   & 7.7M       & 125         & -       \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[!t]
\centering
\caption{\label{tbl:ppl-wikitext2}
Perplexities on the \texttt{wikitext2} dataset.
The FF model is a 256-dim 2-layer feedforward neural network
with a window of 4 previous tokens with 0.3 dropout.
The LSTM is a 256-dim 2-layer recurrent neural network with 0.3 dropout.
The HMM is a 32k-state HMM with 0.5 state dropout.
}
\begin{tabular}{llll}
\toprule
Model & Num params & Valid PPL & Test PPL\\
\midrule
FF    &            & 209       & -       \\
LSTM  &            & 125       & -       \\
HMM   &            & 167       & -       \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Sparse emission constraint ablation}
We ablate the emission sparsity constraints in Table \ref{tbl:ppl-assn-ablation},
and find that the Brown emission constraints outperforms the uniform emission constraints
in all model sizes.

One explanation for the relative benefit of Brown emission constraints over uniform
is due to the effect of Brown clusters.
The goal of Brown clustering is to place two words in the same cluster
if they are used in the same context.
In Table \ref{tbl:entropy}, we observe that the entropy of the emission distribution
for models with uniform emission
constraints is lower than models with brown constraints,
and the entropy of the transition distributions is higher.
This implies that the burden of modeling ambiguity is pushed onto
the transition distribution,
since the uniform models are less likely to place words that appear in
similar contexts together.

% explain why this hurts the model? or is it already clear
% compare entropies to exact 1k HMM.
% probably find that brown is closer to exact. does this hold true for the 4 brown cluster HMM?

% Include separate table with emission and transition entropies
% am i computing those correctly?

%Analysis
%\begin{itemize}
%\item Hypothesis: Brown Cluster assigns words that are emit in similar contexts.
%Uniform assignment with too few states per word will not group words together
%resulting in needing more states.
%\item Evidence: check entropy of emission distribution / Uniform should be lower entropy.
%\item Conclusion: Suboptimal assignments lead to needing more states
%to achieve the same performance.
%\end{itemize}

\begin{table}[!t]
\centering
\caption{\label{tbl:ppl-assn-ablation}
The perplexities for the different emission sparsity constraints
in a 1024 state HMM as well as larger HMMs for which exact
inference without sparsity is too expensive.
The quantities $|\mcZ|$ and $k$ are the number of hidden
states and the number of states per word respectively.
The HMMs with 1024 states do not have any dropout,
while the 8k and 16k state HMMs have unstructured dropout
at a rate of 0.5.
}

\begin{tabular}{llll}
\toprule
Constraint & $|\mcZ|$ & $k$    & Valid PPL\\
\midrule
Uniform    & 1k       & 32     & -\\
Brown      & 1k       & 32     & -\\
None       & 1k       & 1k     & -\\
\midrule 
Uniform    & 8k       & 128    & 150*\\
Brown      & 8k       & 128    & 142*\\
Uniform    & 16k      & 128    & 146*\\
Brown      & 16k      & 128    & 134*\\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[!t]
\centering
\caption{\label{tbl:entropy}
The average entropies of the the emission and
transition distributions for HMMs
with uniform and Brown cluster emission constraints.
All models have $k=128$ states per word
and use unstructured dropout with a rate of $p=0.5$.
}

\begin{tabular}{llll}
\toprule
Constraint & $|\mcZ|$ & H(emit)& H(transition)\\
\midrule 
Uniform    & 8k       & -      & - \\
Brown      & 8k       & -      & - \\
Uniform    & 16k      & -      & - \\
Brown      & 16k      & -      & - \\
\bottomrule
\end{tabular}
\end{table}


\paragraph{Dropout analysis}
The effect of the different dropout strategies and rates is 
shown in Table~\ref{tbl:dropout}.
We found that state dropout outperformed unstructured dropout overall.
This in addition to the computational benefits afforded by state dropout
led us to prefer this strategy.

Unstructured dropout was sensitive to batching strategies.
We observed a larger gap between training and validation perplexities
with the unshuffled batching strategy,
whereas unstructured dropout appeared to be robust to the batching strategy.

Analysis 1: Discuss what happens with just state dropout, without partitioning.
(Too much variance in gradient estimator, would require variance reduction.
Get numbers or just handwaive?
Consider exact inference (in constrained model) this work.
)

\begin{table}[!t]
\centering
\caption{\label{tbl:dropout}
State occupancies for the dropout strategies and rates.
All models were HMMs with Brown cluster emission constraints,
16k total states, and 128 states per word (and therefore 128 Brown clusters).
}

\begin{tabular}{llll}
\toprule
Dropout type & $p$  & Valid PPL & Occupancy\\
\midrule 
Unstructured & 0.25 & -         & - \\
Unstructured & 0.5  & -         & - \\
Unstructured & 0.75 & -         & - \\
State        & 0.25 & -         & - \\
State        & 0.5  & -         & - \\
State        & 0.75 & -         & - \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Number of Brown clusters}
We next examine the sensitivity of HMMs to the number of Brown clusters
in Table \ref{tbl:ppl-spw-ablation}.
We find that models at with 16k or 32k total states
are not sensitive to the number of Brown clusters
in the range where marginalization is computationally feasible.
This contrasts with the observation that the number of Brown clusters
influenced performance at 1k total states.
% FILL IN HERE, reconcile differences

\begin{table}[!t]
\centering
\caption{\label{tbl:ppl-spw-ablation}
The perplexities for HMMs with 16k states
and different numbers of Brown clusters
for constraining the emission distribution of the HMMs.
$|\mcZ|$ is the total number of hidden states.
All models have 0.5 state dropout.
}

\begin{tabular}{llll}
\toprule
$|\mcZ|$ & Num clusters & Valid PPL\\
\midrule
16k      & 32           & 136\\
16k      & 64           & 137\\
16k      & 128          & 133\\
16k      & 256          & 137\\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Weight tying ablation}
Analysis 1: Discuss performance using different tying methods
(found this to not affect performance) and computational savings
in terms of number of parameters.
Discuss relative parameter inefficiency compared to LSTM / FF.

\paragraph{State size ablation}
We find that as we increase the number of total states
while keeping the Brown clusters fixed,
performance improves up until we have 65k states.

Analysis 1: How does state usage change as clusters remain constant
but the number of states (and states per word) increases?

\begin{table}[!t]
\centering
\caption{\label{tbl:ppl-states-ablation}
The perplexities for HMMs with 128 Brown clusters
for constraining the emission distribution of the HMMs.
$|\mcZ|$ is the total number of hidden states.
All models have 0.5 state dropout.
}

\begin{tabular}{llll}
\toprule
$|\mcZ|$ & Num clusters & Valid PPL\\
\midrule
16k      & 128          & 133\\
32k      & 256          & 126\\
65k      & 512          & 124\\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Parameterization ablation}
Analysis 1: Is there a performance difference between neural / scalar parameterization,
and is it consistent across state sizes?

Discussion 1: Memory savings when working with state dropout to not instantiate the
full matrices during training, which makes working with a neural parameterization
beneficial.

\subsection{Error analysis}
\paragraph{}
%\paragraph{}

\section{Discussion}

%\subsection{POS Induction}
%\subsection{Word sense induction}

\section*{Acknowledgments}
TBD

\bibliographystyle{acl_natbib}
\bibliography{anthology,emnlp2020}

\appendix

\section{Hyperparameters}
\label{sec:hyperparams}
LSTM
\begin{itemize}
\item 2 layers
\end{itemize}

\begin{comment}
\section{Parameter estimation}
We maximize the evidence of the observed sentences
$\log p(\bx) = \log \sum_{\bz} p(\bx, \bz)$
via gradient ascent.

\subsection{Gradient of the evidence}
Let $\psi_0(z_0, z_1) = \log p(x_0, z_0)$ and
$\psi_t(z_{t}, z_{t+1}) = \log p(x_{t+1}, z_{t+1} \mid z_{t})$ for $t \in \set{1, \ldots, T-1}$.
Additionally, let $\oplus$ and $\otimes$ be addition and multiplication in the log semiring.
After conditioning on the observed $\bx$, we can express the evidence as the following:
\begin{equation}
\label{eqn:evidence}
A_x = \log p(\bx) = \bigoplus_{\bz}\bigotimes_{t=0}^{T-1} \psi_t(z_t, z_{t+1})
\end{equation}
where $A_\bx$ is the clamped log partition function (after observing $\bx_{0:T}$).

We show that the gradient of the log partition function with respect to the $\psi_t(z_t, z_{t+1})$
is the first moment (a general result for the cumulant generating function
of exponential family distributions). Given the value of the derivative
$\frac{\partial A_\bx}{\partial \psi_t(z_t, z_{t+1})}$,
we can then apply the chain rule to compute the total derivative with respect to
downstream parameters. For example, the gradient with respect to the transition matrix
of the HMM is given by
$$\frac{\partial A_\bx}{\partial \theta_{ij}}
= \sum_t \frac{\partial A_\bx}{\partial \psi_t(i,j)}
\frac{\partial \psi_t(i,j)}{\partial \theta_{ij}}$$

Recall that the derivative of logaddexp ($\oplus$ in the log semiring) is
\begin{equation}
\label{eqn:lse_derivative}
\begin{aligned}
\frac{\partial}{\partial x} x \oplus y
&= \frac{\partial}{\partial x} \log e^x + e^ y\\
&= \frac{e^x}{e^x + e^y}\\
&= \exp(x - (x \oplus y))
\end{aligned}
\end{equation}
while the derivative of addition ($\otimes$ in the log semiring) is
\begin{equation}
\label{eqn:plus_derivative}
\frac{\partial}{\partial x} x \otimes y = 1
\end{equation}

%(TODO name variables to make this readable in 2col format)
The derivative of the clamped log partition function $A_\bx$ is given by
\begin{align*}
&\frac{\partial A_\bx}{\partial \psi_t(i,j)}\\
&= \frac{\partial}{\partial \psi_t(i,j)} \bigoplus_{\bz}
    \bigotimes_{t'} \psi_{t'}(z_{t'}, z_{t'+1})\\
&= \frac{\partial}{\partial \psi_t(i,j)} \Bigg(
        \Bigg(\bigoplus_{\bz:z_t = i, z_{t+1} = j}
        \bigotimes_{t'} \psi_{t'}(z_{t'}, z_{t'+1})\Bigg)\\
& \qquad \bigoplus
        \Bigg(\bigoplus_{\bz:z_t \ne i, z_{t+1} \ne j}
        \bigotimes_{t'} \psi_{t'}(z_{t'}, z_{t'+1})\Bigg)
    \Bigg)\\
&= \exp\Bigg(\Bigg(\bigoplus_{\bz:z_t = i, z_{t+1} = j} 
    \bigotimes_{t'} \psi_{t'}(z_{t'}, z_{t'+1})\Bigg)\\
&\qquad - \Bigg(\Bigg(\bigoplus_{\bz:z_t = i, z_{t+1} = j}
        \bigotimes_{t'} \psi_{t'}(z_{t'}, z_{t'+1})\Bigg)\\
    &\qquad \qquad \bigoplus
        \Bigg(\bigoplus_{\bz:z_t \ne i, z_{t+1} \ne j}
        \bigotimes_{t'} \psi_{t'}(z_{t'}, z_{t'+1})\Bigg)
    \Bigg)\Bigg)\\
&= \exp\left(\left(\bigoplus_{\bz:z_t = i, z_{t+1} = j} 
    \bigotimes_{t'} \psi_{t'}(z_{t'}, z_{t'+1})\right)
    - A_\bx\right)
\end{align*} 
which is the value at $i$ and $j$ of the
edge marginal for $z_t, z_{t+1}$ obtained via the forward-backward algorithm.
The second equality is a result of the associativity of $\oplus$;
the third equality is a result of Eqn. \ref{eqn:lse_derivative};
and the fourth equality from Eqn. \ref{eqn:plus_derivative}.
\end{comment}


\section{Supplemental Material}
\label{sec:supplemental}
\end{document}
