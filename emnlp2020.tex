%
% File emnlp2020.tex
%
%% Based on the style files for ACL 2020, which were
%% Based on the style files for ACL 2018, NAACL 2018/19, which were
%% Based on the style files for ACL-2015, with some improvements
%%  taken from the NAACL-2016 style
%% Based on the style files for ACL-2014, which were, in turn,
%% based on ACL-2013, ACL-2012, ACL-2011, ACL-2010, ACL-IJCNLP-2009,
%% EACL-2009, IJCNLP-2008...
%% Based on the style files for EACL 2006 by 
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{emnlp2020}
\usepackage{times}
\usepackage{latexsym}
\renewcommand{\UrlFont}{\ttfamily\small}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

\usepackage{mystyle}

%\aclfinalcopy % Uncomment this line for the final submission
%\def\aclpaperid{***} %  Enter the acl Paper ID here

%\setlength\titlebox{5cm}
% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.

\newcommand\BibTeX{B\textsc{ib}\TeX}

\title{Instructions for EMNLP 2020 Proceedings}

\author{First Author \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  \texttt{email@domain} \\\And
  Second Author \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  \texttt{email@domain} \\}

\date{}

\begin{document}
\maketitle
\begin{abstract}
todo
\end{abstract}

\section{Introduction}

todo


\section{Background}
We focus on language modeling,
where we learn a distribution over sequences of tokens
at the word level $\bx = \set{x_0, \ldots, x_T}$, with each token $x_t$
an element of the finite vocabulary $\mcX$.

\subsection{Hidden Markov Models}
Hidden Markov Models (HMMs) specify a joint distribution over 
the observed words $\bx$ and discrete latent states $\bz = \set{z_0, \ldots, z_T}$,
each $z_t$ from finite set $\mcZ$,
with the following generative process:
For every time step $t \in \set{0,\ldots,T}$, choose a state given the previous state
$z_t \mid z_{t-1}$ from the transition distribution $p(z_t \mid z_{t-1})$,
where the first state $z_0$ is chosen from the starting distribution $p(z_0)$.
Then choose a word given the current state $x_t \mid z_t$ from the emission distribution $p(x_t \mid z_t)$.
This yields the joint distribution
\begin{equation}
p(\bx, \bz)
= p(z_0)p(x_0 \mid z_0)\prod_{t=1}^T p(x_t\mid z_t)p(z_t \mid z_{t-1})
\end{equation}

%By virtue of modeling the joint distribution over $\bx_{0:T},\bz_{0:T}$,
%HMMs maintain uncertainty over both observed words the unobserved latent states.
%Contrast this with another class of language models, recurrent neural networks (RNNs),
%which only maintain uncertainty over the observed $\bx_{0:T}$.

%At every timestep $t$, the generative process bottlenecks all information from the past
%through a single discrete state $z_t$ from a finite set.
%This contrasts 

The distributions are parameterized as follows
\begin{equation}
\label{param}
\begin{aligned}
p(z_0) &\propto e^{\phi_{z_0}}\\
p(z_t \mid z_{t-1}) &\propto e^{\phi_{z_tz_{t-1}}}\\
p(x_t \mid z_t) &\propto e^{\phi_{x_tz_t}}
\end{aligned}
\end{equation}
where each $\phi_{z_0},\phi_{z_tz_{t-1}},\phi_{x_tz_t} \in \mathbb{R}$
may be parameterized by a scalar or a neural network.

\section{Model}
For language modeling, we are interested in the distribution over the observed words
obtained by marginalizing over the latent states $p(\bx) = \sum_{\bz}p(\bx,\bz)$.
This sum can be computed in time $O(T|\mcZ|^2)$ via dynamic programming,
which becomes prohibitive if the number of latent states $|\mcZ|$ is large.
However, in order to increase the capacity of HMMs, we must consider large $\mcZ$.
We outline several techniques to manage the computational complexity of marginalization in HMMs.  

\subsection{Sparse Emission HMMs}
We introduce a sparse emission constraint that allows us to
efficiently perform conditional inference in large state spaces.
(TODO: clarify difference in related work. should we also run this as a baseline?
intuition: not enough parameter sharing if each state only emits a single word.
can we prove that you need more states if the emission distribution is overconstrained?)
Inspired by Cloned HMMs \citep{dedieu2019learning},
we constrain each word $x$ to be emit only by $z \in \mcC_x \subset \mcZ$:
\begin{equation}
\label{eqn:sparse_emission}
p(x \mid z) \propto \begin{cases}
e^{\phi_{zx}} & z \in \mcC_x \\
0 & \textrm{otherwise}
\end{cases}
\end{equation}
This allows us to perform marginalization as follows
\begin{equation}
\label{eqn:sparse_marginalization}
\begin{aligned}
p(\bx) &= \sum_{z_0} p(z_0)p(x_0 \mid z_0)
    \sum_{z_1}p(z_1\mid z_0)p(x_1 \mid z_1)\\
    &\qquad\cdots
    \sum_{z_T}p(z_T \mid z_{T-1})p(x_T \mid z_T)\\
&= \sum_{z_0 \in \mcC_{x_0}} p(z_0)p(x_0 \mid z_0)
    \sum_{z_1 \in \mcC_{x_1}} p(z_1\mid z_0)p(x_1 \mid z_1)\\
    &\qquad\cdots
    \sum_{z_T \in \mcC_{x_T}} p(z_T \mid z_{T-1})p(x_T \mid z_T)
\end{aligned}
\end{equation}
which takes $O(T \max_x(|\mcC_x|)^2)$ computation.

We experiment with two constraint sets, both of which are specified 
as hyperparameters and remain fixed for a given model.
The first constraint set is obtained by sampling subsets of
states for each word i.i.d. from a uniform distribution over subsets.
The second constraint set uses Brown Clusters \citep{brown1992}
to assign all words in a given Brown cluster the same subset of states.
This constraint set is advantageous, as it admits further optimizations
that we discuss below.

\subsection{State Dropout}
We introduce a variant of dropout called state dropout that operates on the
start and transition distributions.
The goal of state dropout is to reduce the complexity of marginalization in a manner
identical to the sparse emission constraints.

State dropout samples a mask over state partitions
$\mathbf{b}_\mcC \in \set{0,1}^{|\mcC|}$ for each partition $\mcC \subset \mcZ$,
such that each sampled $|\mathbf{b}_\mcC| = k$.
(TODO: Mention multivariate hypergeometric?)
We only apply this if the $\mcC_x$ are disjoint partitions,
as with the Brown constraint set.

Let $\mathbf{b}$ be the concatenation of the $\mathbf{b}_\mcC$ for all partitions $\mcC$,
such that $\mathbf{b}_z = 1$ if $\mathbf{b}_{\mcC z} = 1$, where $z \in \mcC$.
The start and transition distributions with dropout are given by
\begin{equation}
\label{eqn:state_dropout}
\begin{aligned}
p(z_0) &\propto \begin{cases}
e^{\phi_{z_0}} & \mathbf{b}_{z_0} = 1\\
0 & \textrm{otherwise}\\
\end{cases}\\
p(z_t \mid z_{t-1}) &\propto \begin{cases}
e^{\phi_{z_tz_{t-1}}} & \mathbf{b}_{z_t} = 1 \wedge \mathbf{b}_{z_{t-1}} = 1\\
0 & \textrm{otherwise}
\end{cases}
\end{aligned}
\end{equation}

Marginalizing over $\bz$ with state dropout requires $O(Tk^2)$ computation,
where $k$ is the number of nonzero elements of $\mathbf{b}_\mcC, \forall \mcC$.

\subsection{Learning}
We optimize the evidence of observed sentences $\log p(\bx) = \log \sum_\bz p(\bx,\bz)$
by first marginalizing over latent states $\bz$ then performing gradient ascent.
The emission sparsity constraints we introduce as well as state dropout
allow us to both perform marginalization
and compute the gradient of the evidence efficiently.

\section{Experiments}
\subsection{Language Modeling}
\subsubsection{Datasets}
\paragraph{Penn Treebank}
\paragraph{WikiText-2}
\subsubsection{Baselines}

\subsubsection{Results}
Only PTB results for now.
\paragraph{Sparse emission constraint ablation}
Graph 1: Show Brown generalizes better than uniform in small model,
smaller performance relative loss than uniform compared to exact HMM.
\begin{itemize}
\item Uniform, no dropout, 1k states (vary states per word)
\item Brown, no dropout, 1k states (vary states per word)
\item No sparsity, no dropout, 1k states
\end{itemize}

Graph 2: Show Brown continues to generalize better than uniform in larger states.
\begin{itemize}
\item Uniform, 16k states (vary states per word), unstructured dropout
\item Brown, 16k states (vary states per word), unstructured dropout
\end{itemize}

\paragraph{Dropout ablation}
Graph 1: Show dropout helps but variants don't affect performance too much.
Justify state dropout as the one with simplest implementation.
\begin{itemize}
\item Brown + unstructured dropout
\item Brown + state dropout
\end{itemize}

Analysis 1: Discuss what happens with just state dropout, without partitioning.

\paragraph{Effective compute ($k$) ablation}
Graph 1: Fix the total number of states and examine sensitivity
to the number of brown clusters / states per word.
\begin{itemize}
\item 16k states, 32 clusters
\item 16k states, 64 clusters
\item 16k states, 128 clusters
\end{itemize}

\paragraph{Weight tying ablation}
Analysis 1: Discuss performance using different tying methods
(found this to not affect performance) and computational savings
in terms of number of parameters.
Discuss relative parameter inefficiency compared to LSTM / FF.

\paragraph{State size ablation}
Graph 1: Show performance improves as we increase the total number of states
\begin{itemize}
\item Brown HMM + state dropout, 16k, 128 clusters
\item Brown HMM + state dropout, 32k, 128 clusters
\item Brown HMM + state dropout, 64k, 128 clusters
\end{itemize}

Analysis 1: How does state usage change as clusters remain constant
but the number of states (and states per word) increases?

\paragraph{Parameterization ablation}
Analysis 1: Is there a performance difference between neural / scalar parameterization,
and is it consistent across state sizes?

Discussion 1: Memory savings when working with state dropout to not instantiate the
full matrices during training, which makes working with a neural parameterization
beneficial.

%\paragraph{}

%\subsection{POS Induction}
%\subsection{Word sense induction}

\section*{Acknowledgments}
TBD

\bibliographystyle{acl_natbib}
\bibliography{anthology,emnlp2020}

\appendix

\section{Parameter estimation}
We maximize the evidence of the observed sentences
$\log p(\bx) = \log \sum_{\bz} p(\bx, \bz)$
via gradient ascent.

\subsection{Gradient of the evidence}
Let $\psi_0(z_0, z_1) = \log p(x_0, z_0)$ and
$\psi_t(z_{t}, z_{t+1}) = \log p(x_{t+1}, z_{t+1} \mid z_{t})$ for $t \in \set{1, \ldots, T-1}$.
Additionally, let $\oplus$ and $\otimes$ be addition and multiplication in the log semiring.
After conditioning on the observed $\bx$, we can express the evidence as the following:
\begin{equation}
\label{eqn:evidence}
A_x = \log p(\bx) = \bigoplus_{\bz}\bigotimes_{t=0}^{T-1} \psi_t(z_t, z_{t+1})
\end{equation}
where $A_\bx$ is the clamped log partition function (after observing $\bx_{0:T}$).

We show that the gradient of the log partition function with respect to the $\psi_t(z_t, z_{t+1})$
is the first moment (a general result for the cumulant generating function
of exponential family distributions). Given the value of the derivative
$\frac{\partial A_\bx}{\partial \psi_t(z_t, z_{t+1})}$,
we can then apply the chain rule to compute the total derivative with respect to
downstream parameters. For example, the gradient with respect to the transition matrix
of the HMM is given by
$$\frac{\partial A_\bx}{\partial \theta_{ij}}
= \sum_t \frac{\partial A_\bx}{\partial \psi_t(i,j)}
\frac{\partial \psi_t(i,j)}{\partial \theta_{ij}}$$

Recall that the derivative of logaddexp ($\oplus$ in the log semiring) is
\begin{equation}
\label{eqn:lse_derivative}
\begin{aligned}
\frac{\partial}{\partial x} x \oplus y
&= \frac{\partial}{\partial x} \log e^x + e^ y\\
&= \frac{e^x}{e^x + e^y}\\
&= \exp(x - (x \oplus y))
\end{aligned}
\end{equation}
while the derivative of addition ($\otimes$ in the log semiring) is
\begin{equation}
\label{eqn:plus_derivative}
\frac{\partial}{\partial x} x \otimes y = 1
\end{equation}

(TODO name variables to make this readable in 2col format)
The derivative of the clamped log partition function $A_\bx$ is given by
\begin{align*}
&\frac{\partial A_\bx}{\partial \psi_t(i,j)}\\
&= \frac{\partial}{\partial \psi_t(i,j)} \bigoplus_{\bz}
    \bigotimes_{t'} \psi_{t'}(z_{t'}, z_{t'+1})\\
&= \frac{\partial}{\partial \psi_t(i,j)} \Bigg(
        \Bigg(\bigoplus_{\bz:z_t = i, z_{t+1} = j}
        \bigotimes_{t'} \psi_{t'}(z_{t'}, z_{t'+1})\Bigg)\\
& \qquad \bigoplus
        \Bigg(\bigoplus_{\bz:z_t \ne i, z_{t+1} \ne j}
        \bigotimes_{t'} \psi_{t'}(z_{t'}, z_{t'+1})\Bigg)
    \Bigg)\\
&= \exp\Bigg(\Bigg(\bigoplus_{\bz:z_t = i, z_{t+1} = j} 
    \bigotimes_{t'} \psi_{t'}(z_{t'}, z_{t'+1})\Bigg)\\
&\qquad - \Bigg(\Bigg(\bigoplus_{\bz:z_t = i, z_{t+1} = j}
        \bigotimes_{t'} \psi_{t'}(z_{t'}, z_{t'+1})\Bigg)\\
    &\qquad \qquad \bigoplus
        \Bigg(\bigoplus_{\bz:z_t \ne i, z_{t+1} \ne j}
        \bigotimes_{t'} \psi_{t'}(z_{t'}, z_{t'+1})\Bigg)
    \Bigg)\Bigg)\\
&= \exp\left(\left(\bigoplus_{\bz:z_t = i, z_{t+1} = j} 
    \bigotimes_{t'} \psi_{t'}(z_{t'}, z_{t'+1})\right)
    - A_\bx\right)
\end{align*} 
which is the value at $i$ and $j$ of the
edge marginal for $z_t, z_{t+1}$ obtained via the forward-backward algorithm.
The second equality is a result of the associativity of $\oplus$;
the third equality is a result of Eqn. \ref{eqn:lse_derivative};
and the fourth equality from Eqn. \ref{eqn:plus_derivative}.


\section{Supplemental Material}
\label{sec:supplemental}
\end{document}
