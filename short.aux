\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{merialdo1994tagging}
\citation{vogel1996hmm}
\citation{kuhn1994hmmlm,huang2011thesis}
\citation{zaremba2014lstm}
\citation{bengio2003nlm}
\citation{zaremba2014lstm,merity2017awdlstm}
\citation{buys2018hmm}
\citation{krakovna2016hmm}
\citation{tran2016hmm}
\citation{han2017dependency}
\citation{wiseman2018hsmm}
\citation{kim2019cpcfg}
\newlabel{sec:rw}{{2}{1}{Related Work}{section.2}{}}
\citation{dedieu2019learning}
\citation{petrov2006splitmerge,huang2011thesis}
\citation{huang2011thesis}
\citation{buys2018hmm}
\citation{ladner1980prefix}
\citation{bradbury2016qrnn}
\citation{jin2020discrete}
\citation{dedieu2019learning}
\newlabel{param}{{2}{2}{Background: HMMs}{equation.3.2}{}}
\newlabel{eqn:factors}{{3}{2}{Background: HMMs}{equation.3.3}{}}
\newlabel{eqn:mat_ev}{{4}{2}{Background: HMMs}{equation.3.4}{}}
\newlabel{sec:methods}{{4}{2}{Scaling HMMs}{section.4}{}}
\citation{kim2019cpcfg}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:emit}{{1}{3}{The emission matrix as a set of blocks $\mathbf {O}_1, \ldots , \mathbf {O}_4$ with fixed height $k$. The width of each block may vary, as there is no constraint on the number of words a state can emit. Each active cell is constructed from word $\mathbf {E}_x$ and state $\mathbf {E}_z$ embeddings. \relax }{figure.caption.1}{}}
\newlabel{eqn:sparse_marginalization}{{5}{3}{Scaling HMMs}{equation.4.5}{}}
\newlabel{eqn:sparse_factors}{{6}{3}{Scaling HMMs}{equation.4.6}{}}
\newlabel{fig:algo}{{1}{3}{HMM Training (a single batch) \relax }{algorithm.1}{}}
\citation{brown1992,liang2005brown}
\citation{vieira2014gumbel}
\citation{ptb}
\citation{wikitext}
\citation{mikolov-2011}
\citation{merity2017awdlstm}
\citation{merity2017awdlstm}
\citation{buys2018hmm}
\citation{mikolov2012rnn,kenlm}
\citation{buys2018hmm}
\citation{buys2018hmm}
\citation{buys2018hmm}
\newlabel{fig:trellis}{{2}{4}{The computation of $p(\bx )$ is greatly reduced by blocked emissions and state dropout. In the above trellis, each row corresponds to a latent state and each column after the first to a timestep. Each edge between nodes corresponds to a nonzero entry of $\bF _t^d$. Blocked emissions result in a small subset of all states emitting a given word, as shown by the rectangles, thus only states in the rectangles may occur with nonzero probability after conditioning on observed words. State dropout (leftmost column) allows us to further reduce the number of states we consider, halving the number of (white) states that have nonzero probability in each rectangle. \relax }{figure.caption.2}{}}
\newlabel{eqn:dropout_factors}{{8}{4}{Scaling HMMs}{equation.4.8}{}}
\newlabel{sec:experiments}{{5}{4}{Experimental Setup}{section.5}{}}
\bibstyle{acl_natbib}
\bibdata{anthology,emnlp2020}
\bibcite{bengio2003nlm}{{1}{2003}{{Bengio et~al.}}{{Bengio, Ducharme, Vincent, and Janvin}}}
\bibcite{bradbury2016qrnn}{{2}{2016}{{Bradbury et~al.}}{{Bradbury, Merity, Xiong, and Socher}}}
\bibcite{brown1992}{{3}{1992}{{Brown et~al.}}{{Brown, deSouza, Mercer, Pietra, and Lai}}}
\newlabel{tbl:ppl}{{1}{5}{Perplexities on \texttt {PTB / Wikitext-2}. \relax }{table.caption.3}{}}
\newlabel{tbl:states-ablation}{{3}{5}{Perplexity on \texttt {PTB} by state size $|\mcZ |$ ($\lambda =0.5$ and $M=128$). \relax }{figure.caption.4}{}}
\newlabel{tbl:dropout-param-ablation}{{2}{5}{Ablations on \texttt {PTB} ($\lambda =0.5$ and $M=128$). Time is ms per eval batch (Run on RTX 2080). \relax }{table.caption.5}{}}
\bibcite{buys2018hmm}{{4}{2018}{{Buys et~al.}}{{Buys, Bisk, and Choi}}}
\bibcite{tvm}{{5}{2018}{{Chen et~al.}}{{Chen, Moreau, Jiang, Shen, Yan, Wang, Hu, Ceze, Guestrin, and Krishnamurthy}}}
\bibcite{dedieu2019learning}{{6}{2019}{{Dedieu et~al.}}{{Dedieu, Gothoskar, Swingle, Lehrach, L\IeC {\'a}zaro-Gredilla, and George}}}
\bibcite{han2017dependency}{{7}{2017}{{Han et~al.}}{{Han, Jiang, and Tu}}}
\bibcite{kenlm}{{8}{2013}{{Heafield et~al.}}{{Heafield, Pouzyrevsky, Clark, and Koehn}}}
\bibcite{huang2011thesis}{{9}{2011}{{Huang}}{{}}}
\bibcite{jin2020discrete}{{10}{2020}{{Jin et~al.}}{{Jin, Wiseman, Stratos, and Livescu}}}
\bibcite{kim2019cpcfg}{{11}{2019}{{Kim et~al.}}{{Kim, Dyer, and Rush}}}
\bibcite{krakovna2016hmm}{{12}{2016}{{Krakovna and Doshi-Velez}}{{}}}
\bibcite{kuhn1994hmmlm}{{13}{1994}{{Kuhn et~al.}}{{Kuhn, Niemann, and Schukat-Talamazzini}}}
\bibcite{ladner1980prefix}{{14}{1980}{{Ladner and Fischer}}{{}}}
\bibcite{liang2005brown}{{15}{2005}{{Liang}}{{}}}
\bibcite{adamw}{{16}{2017}{{Loshchilov and Hutter}}{{}}}
\bibcite{ptb}{{17}{1993}{{Marcus et~al.}}{{Marcus, Santorini, and Marcinkiewicz}}}
\bibcite{merialdo1994tagging}{{18}{1994}{{Merialdo}}{{}}}
\bibcite{merity2017awdlstm}{{19}{2017}{{Merity et~al.}}{{Merity, Keskar, and Socher}}}
\bibcite{wikitext}{{20}{2016}{{Merity et~al.}}{{Merity, Xiong, Bradbury, and Socher}}}
\bibcite{mikolov2012rnn}{{21}{2012}{{{Mikolov} and {Zweig}}}{{}}}
\bibcite{mikolov-2011}{{22}{2011}{{Mikolov et~al.}}{{Mikolov, Deoras, Kombrink, Burget, and Cernock\IeC {\'y}}}}
\bibcite{petrov2006splitmerge}{{23}{2006}{{Petrov et~al.}}{{Petrov, Barrett, Thibaux, and Klein}}}
\bibcite{tran2016hmm}{{24}{2016}{{Tran et~al.}}{{Tran, Bisk, Vaswani, Marcu, and Knight}}}
\bibcite{vieira2014gumbel}{{25}{2014}{{Vieira}}{{}}}
\bibcite{vogel1996hmm}{{26}{1996}{{Vogel et~al.}}{{Vogel, Ney, and Tillmann}}}
\bibcite{wiseman2018hsmm}{{27}{2018}{{Wiseman et~al.}}{{Wiseman, Shieber, and Rush}}}
\bibcite{zaremba2014lstm}{{28}{2014}{{Zaremba et~al.}}{{Zaremba, Sutskever, and Vinyals}}}
\citation{adamw}
\citation{adamw}
\citation{tvm}
\newlabel{eqn:mat_ev}{{10}{7}{Inference and Learning in HMMs}{equation.A.10}{}}
\newlabel{sec:hyperparams}{{A.3}{7}{Hyperparameters}{subsection.A.3}{}}
\newlabel{eqn:res}{{12}{7}{HMM Parameterization}{equation.A.12}{}}
\newlabel{tbl:constraint-ablation}{{3}{8}{Emission constraint ablations on \texttt {Penn Treebank}. \relax }{table.caption.8}{}}
\newlabel{fig:fac-ablation}{{4}{9}{Perplexity on \texttt {PTB} by number of blocks $M$ ($\lambda =0.5$ and $|\mcZ |=2^{14}$). \relax }{figure.caption.9}{}}
\newlabel{tbl:dropout-param-ablation-repeat}{{4}{9}{Ablations on \texttt {PTB} ($\lambda =0.5$ and $M=128$). Time is ms per eval batch (Run on RTX 2080). \relax }{table.caption.10}{}}
