\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{rabiner1990tut}
\citation{vogel1996hmm}
\citation{kuhn1994hmmlm,huang2011thesis}
\citation{bengio2003nlm}
\citation{mikolov2010rnn,zaremba2014lstm,merity2017awdlstm}
\citation{radford2019language}
\citation{bengio2003nlm}
\citation{zaremba2014lstm}
\citation{merity2017awdlstm}
\citation{buys2018hmm}
\citation{krakovna2016hmm}
\citation{buys2018hmm}
\citation{krakovna2016hmm}
\citation{tran2016hmm}
\citation{han2017dependency}
\citation{wiseman2018hsmm}
\citation{kim2019cpcfg}
\citation{dedieu2019learning}
\citation{dedieu2019learning}
\citation{brown1992}
\citation{petrov2006splitmerge,huang2011thesis}
\citation{huang2011thesis}
\citation{dedieu2019learning}
\citation{zoubin1997fhmm,nepal2013fhmm}
\citation{kim2019cpcfg}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:hmm}{{1}{2}{An HMM with tokens $x_t$ and states $z_t$. \relax }{figure.caption.1}{}}
\citation{bradbury2016qrnn}
\newlabel{param}{{2}{3}{Background: HMMs}{equation.3.2}{}}
\newlabel{eqn:res}{{3}{3}{Static Parameterization}{equation.4.3}{}}
\citation{dedieu2019learning}
\newlabel{fig:trellis}{{2}{4}{A depiction of the trellis for marginal inference after applying emission sparsity constraints and the state dropout method. The leftmost column depicts the state dropout mask $\mathbf {b}$, which is used to prevent states from being considered during inference. For every token $x_t$, the only incoming edges with nonzero mass are those whose source was not dropped out, i.e. $b_{z_t} = 1$, and is within the previous partition, i.e. $z_{t-1}\in \mcC _{x_{t-1}}$. \relax }{figure.caption.3}{}}
\newlabel{eqn:sparse_emission}{{5}{4}{Inducing Blocked Transitions}{equation.4.5}{}}
\newlabel{eqn:sparse_marginalization}{{6}{4}{Inducing Blocked Transitions}{equation.4.6}{}}
\citation{brown1992}
\newlabel{fig:algo}{{1}{5}{Pseudocode for computing the likelihood of a corpus \relax }{algorithm.1}{}}
\newlabel{eqn:state_dropout}{{7}{5}{Dropout as State Reduction}{equation.4.7}{}}
\newlabel{sec:experiments}{{5}{5}{Experiments}{section.5}{}}
\citation{kim2015charcnn}
\citation{ma2016crf}
\citation{ptb}
\citation{wikitext}
\citation{mikolov-2011}
\citation{ma2016crf}
\citation{mikolov2012rnn}
\citation{mikolov2012rnn}
\citation{mikolov2012rnn}
\citation{zaremba2014lstm}
\citation{zaremba2014lstm}
\citation{merity2017awdlstm}
\citation{merity2017awdlstm}
\citation{buys2018hmm}
\citation{buys2018hmm}
\newlabel{tbl:ptb-ppl}{{1}{7}{Perplexities on the \texttt {Penn Treebank} dataset. The top shows results from previous work, while the bottom shows our results for models with comparable computational cost. In particular, we compare models that have the same asymptotic inference cost: linear in the length of a sequence and quadratic in the hidden dimension. This is $h=256$ for the FF model and LSTM and $|\mcZ | = 256$ for the HMM. \relax }{table.caption.9}{}}
\newlabel{tbl:wt2-ppl}{{2}{7}{Perplexities on the \texttt {Wikitext-2} dataset. We report results from previous work on top, while the bottom shows our results for the same models and hyperparameters applied to \texttt {Penn Treebank}. \relax }{table.caption.10}{}}
\newlabel{tbl:states-ablation}{{3}{7}{Perplexities on the \texttt {Penn Treebank} dataset. Obviously better as a graph. Increase number of states while holding $\mcC _x=256$ and state dropout at 0.5. \relax }{table.caption.12}{}}
\citation{ma2016crf}
\citation{ma2016crf}
\citation{ma2016crf}
\citation{ma2016crf}
\bibstyle{acl_natbib}
\bibdata{anthology,emnlp2020}
\bibcite{bengio2003nlm}{{1}{2003}{{Bengio et~al.}}{{Bengio, Ducharme, Vincent, and Janvin}}}
\bibcite{bradbury2016qrnn}{{2}{2016}{{Bradbury et~al.}}{{Bradbury, Merity, Xiong, and Socher}}}
\bibcite{brown1992}{{3}{1992}{{Brown et~al.}}{{Brown, deSouza, Mercer, Pietra, and Lai}}}
\bibcite{buys2018hmm}{{4}{2018}{{Buys et~al.}}{{Buys, Bisk, and Choi}}}
\bibcite{dedieu2019learning}{{5}{2019}{{Dedieu et~al.}}{{Dedieu, Gothoskar, Swingle, Lehrach, L\IeC {\'a}zaro-Gredilla, and George}}}
\bibcite{zoubin1997fhmm}{{6}{1997}{{Ghahramani and Jordan}}{{}}}
\bibcite{han2017dependency}{{7}{2017}{{Han et~al.}}{{Han, Jiang, and Tu}}}
\bibcite{huang2011thesis}{{8}{2011}{{Huang}}{{}}}
\bibcite{kim2019cpcfg}{{9}{2019}{{Kim et~al.}}{{Kim, Dyer, and Rush}}}
\bibcite{kim2015charcnn}{{10}{2015}{{Kim et~al.}}{{Kim, Jernite, Sontag, and Rush}}}
\bibcite{krakovna2016hmm}{{11}{2016}{{Krakovna and Doshi-Velez}}{{}}}
\bibcite{kuhn1994hmmlm}{{12}{1994}{{Kuhn et~al.}}{{Kuhn, Niemann, and Schukat-Talamazzini}}}
\bibcite{ma2016crf}{{13}{2016}{{Ma and Hovy}}{{}}}
\newlabel{tbl:constraint-ablation}{{4}{8}{Perplexities on the \texttt {Penn Treebank} dataset. Ablate number of brown clusters. Ablate Brown cluster constraints against uniform for 16k state model. Ablate Brown cluster emission constraints against small HMM. Let $m$ be the number of clusters. \relax }{table.caption.14}{}}
\bibcite{ptb}{{14}{1993}{{Marcus et~al.}}{{Marcus, Santorini, and Marcinkiewicz}}}
\bibcite{merity2017awdlstm}{{15}{2017}{{Merity et~al.}}{{Merity, Keskar, and Socher}}}
\bibcite{wikitext}{{16}{2016}{{Merity et~al.}}{{Merity, Xiong, Bradbury, and Socher}}}
\bibcite{mikolov2012rnn}{{17}{2012}{{{Mikolov} and {Zweig}}}{{}}}
\bibcite{mikolov-2011}{{18}{2011}{{Mikolov et~al.}}{{Mikolov, Deoras, Kombrink, Burget, and Cernock\IeC {\'y}}}}
\bibcite{mikolov2010rnn}{{19}{2010}{{Mikolov et~al.}}{{Mikolov, Karafi\IeC {\'a}t, Burget, Cernock\IeC {\'y}, and Khudanpur}}}
\bibcite{nepal2013fhmm}{{20}{2013}{{Nepal and Yates}}{{}}}
\bibcite{petrov2006splitmerge}{{21}{2006}{{Petrov et~al.}}{{Petrov, Barrett, Thibaux, and Klein}}}
\bibcite{rabiner1990tut}{{22}{1990}{{Rabiner}}{{}}}
\bibcite{radford2019language}{{23}{2019}{{Radford et~al.}}{{Radford, Wu, Child, Luan, Amodei, and Sutskever}}}
\bibcite{tran2016hmm}{{24}{2016}{{Tran et~al.}}{{Tran, Bisk, Vaswani, Marcu, and Knight}}}
\bibcite{vogel1996hmm}{{25}{1996}{{Vogel et~al.}}{{Vogel, Ney, and Tillmann}}}
\bibcite{wiseman2018hsmm}{{26}{2018}{{Wiseman et~al.}}{{Wiseman, Shieber, and Rush}}}
\bibcite{zaremba2014lstm}{{27}{2014}{{Zaremba et~al.}}{{Zaremba, Sutskever, and Vinyals}}}
\newlabel{tbl:dropout-param-ablation}{{5}{9}{Perplexities on the \texttt {Penn Treebank} dataset. Dropout and parameterization ablation \relax }{table.caption.16}{}}
\newlabel{tbl:pos}{{6}{9}{Tagging accuracies on the Wall Street Journal (WSJ) portion of the \texttt {Penn Treebank}. \relax }{table.caption.17}{}}
\newlabel{sec:hyperparams}{{A}{9}{Hyperparameters}{appendix.A}{}}
\newlabel{sec:supplemental}{{B}{9}{Supplemental Material}{appendix.B}{}}
\newlabel{LastPage}{{}{9}{}{page.9}{}}
\xdef\lastpage@lastpage{9}
\xdef\lastpage@lastpageHy{9}
