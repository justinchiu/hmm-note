\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{rabiner1990tut}
\citation{vogel1996hmm}
\citation{kuhn1994hmmlm,huang2011thesis}
\citation{bengio2003nlm}
\citation{mikolov2010rnn,zaremba2014lstm,merity2017awdlstm}
\citation{radford2019language}
\citation{bengio2003nlm}
\citation{zaremba2014lstm}
\citation{merity2017awdlstm}
\citation{buys2018hmm}
\citation{krakovna2016hmm}
\citation{tran2016hmm}
\citation{han2017dependency}
\citation{wiseman2018hsmm}
\citation{kim2019cpcfg}
\citation{dedieu2019learning}
\citation{brown1992}
\citation{petrov2006splitmerge,huang2011thesis}
\citation{huang2011thesis}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:hmm}{{1}{2}{An HMM with tokens $x_t$ and states $z_t$. \relax }{figure.caption.1}{}}
\newlabel{param}{{2}{2}{Background: HMMs}{equation.3.2}{}}
\citation{ladner1980prefix}
\citation{bradbury2016qrnn}
\newlabel{eqn:res}{{4}{3}{}{equation.4.4}{}}
\citation{dedieu2019learning}
\newlabel{eqn:sparse_emission}{{8}{4}{Inducing Blocked Transitions}{equation.4.8}{}}
\newlabel{eqn:sparse_marginalization}{{9}{4}{Inducing Blocked Transitions}{equation.4.9}{}}
\newlabel{fig:trellis}{{2}{4}{HMM search space after applying emission sparsity constraints and the state dropout method. The leftmost column depicts the state dropout mask $\mathbf {b}$, which is used to prevent states from being considered during inference. For every token $x_t$, the only incoming edges with nonzero mass are those whose source was not dropped out, i.e. $b_{z_t} = 1$, and is within the previous partition, i.e. $z_{t-1}\in \mcC _{x_{t-1}}$. The states are partitioned into groups of 8, with the observation partitions $\rho (x_1) = 1, \rho (x_2) = 2,$ and so on. \relax }{figure.caption.3}{}}
\newlabel{eqn:state_dropout}{{10}{4}{Dropout as State Reduction}{equation.4.10}{}}
\citation{brown1992}
\citation{kim2015charcnn}
\citation{ma2016crf}
\newlabel{fig:algo}{{1}{5}{Pseudocode for computing the likelihood of a corpus with state dropout \relax }{algorithm.1}{}}
\newlabel{sec:experiments}{{5}{5}{Experiments}{section.5}{}}
\citation{ptb}
\citation{wikitext}
\citation{mikolov-2011}
\citation{ma2016crf}
\citation{mikolov2012rnn}
\citation{mikolov2012rnn}
\citation{mikolov2012rnn}
\citation{mikolov2012rnn}
\citation{zaremba2014lstm}
\citation{zaremba2014lstm}
\citation{merity2017awdlstm}
\citation{merity2017awdlstm}
\citation{buys2018hmm}
\citation{buys2018hmm}
\newlabel{tbl:states-ablation}{{3}{6}{Perplexities on the \texttt {Penn Treebank} dataset as a function of the state size $|\mcZ |$. We hold the emission constraints fixed using 128 Brown clusters, and state dropout at 0.5. \relax }{figure.caption.9}{}}
\citation{ma2016crf}
\citation{ma2016crf}
\citation{ma2016crf}
\citation{ma2016crf}
\citation{ma2016crf}
\newlabel{tbl:ptb-ppl}{{1}{7}{Perplexities on the \texttt {Penn Treebank} dataset. The top shows results from previous work, while the bottom shows our results for models with comparable computational cost. In particular, we compare models that have the same asymptotic inference cost: linear in the length of a sequence and quadratic in the hidden dimension. This is $h=256$ for the FF model and LSTM and $|\mcZ | = 256$ for the HMM. \relax }{table.caption.7}{}}
\newlabel{tbl:constraint-ablation}{{2}{7}{Perplexities on the \texttt {Penn Treebank} dataset. We ablate the effect of the number of Brown clusters, examine whether there may be a drop in performance due to the emission sparsity constraint, and compare the Brown cluster constraint to a uniform baseline. All models have 0.5 state dropout, except for the 1k state HMMs, which have no dropout. We use $m$ to indicate the number of clusters. \relax }{table.caption.11}{}}
\bibstyle{acl_natbib}
\bibdata{anthology,emnlp2020}
\bibcite{bengio2003nlm}{{1}{2003}{{Bengio et~al.}}{{Bengio, Ducharme, Vincent, and Janvin}}}
\bibcite{bradbury2016qrnn}{{2}{2016}{{Bradbury et~al.}}{{Bradbury, Merity, Xiong, and Socher}}}
\bibcite{brown1992}{{3}{1992}{{Brown et~al.}}{{Brown, deSouza, Mercer, Pietra, and Lai}}}
\bibcite{buys2018hmm}{{4}{2018}{{Buys et~al.}}{{Buys, Bisk, and Choi}}}
\bibcite{dedieu2019learning}{{5}{2019}{{Dedieu et~al.}}{{Dedieu, Gothoskar, Swingle, Lehrach, L\IeC {\'a}zaro-Gredilla, and George}}}
\bibcite{han2017dependency}{{6}{2017}{{Han et~al.}}{{Han, Jiang, and Tu}}}
\bibcite{huang2011thesis}{{7}{2011}{{Huang}}{{}}}
\bibcite{kim2019cpcfg}{{8}{2019}{{Kim et~al.}}{{Kim, Dyer, and Rush}}}
\bibcite{kim2015charcnn}{{9}{2015}{{Kim et~al.}}{{Kim, Jernite, Sontag, and Rush}}}
\bibcite{krakovna2016hmm}{{10}{2016}{{Krakovna and Doshi-Velez}}{{}}}
\bibcite{kuhn1994hmmlm}{{11}{1994}{{Kuhn et~al.}}{{Kuhn, Niemann, and Schukat-Talamazzini}}}
\bibcite{ladner1980prefix}{{12}{1980}{{Ladner and Fischer}}{{}}}
\bibcite{ma2016crf}{{13}{2016}{{Ma and Hovy}}{{}}}
\bibcite{ptb}{{14}{1993}{{Marcus et~al.}}{{Marcus, Santorini, and Marcinkiewicz}}}
\bibcite{merity2017awdlstm}{{15}{2017}{{Merity et~al.}}{{Merity, Keskar, and Socher}}}
\newlabel{tbl:dropout-param-ablation}{{3}{8}{We report perplexities on the \texttt {Penn Treebank} dataset for a 16k state HMM with 0.5 state dropout, and ablate dropout and the neural parameterization one at a time. \relax }{table.caption.13}{}}
\newlabel{tbl:pos}{{4}{8}{Tagging accuracies on the Wall Street Journal (WSJ) portion of the \texttt {Penn Treebank}. We use a HMM with 32k states and 0.5 state dropout. \relax }{table.caption.14}{}}
\bibcite{wikitext}{{16}{2016}{{Merity et~al.}}{{Merity, Xiong, Bradbury, and Socher}}}
\bibcite{mikolov2012rnn}{{17}{2012}{{{Mikolov} and {Zweig}}}{{}}}
\bibcite{mikolov-2011}{{18}{2011}{{Mikolov et~al.}}{{Mikolov, Deoras, Kombrink, Burget, and Cernock\IeC {\'y}}}}
\bibcite{mikolov2010rnn}{{19}{2010}{{Mikolov et~al.}}{{Mikolov, Karafi\IeC {\'a}t, Burget, Cernock\IeC {\'y}, and Khudanpur}}}
\bibcite{petrov2006splitmerge}{{20}{2006}{{Petrov et~al.}}{{Petrov, Barrett, Thibaux, and Klein}}}
\bibcite{rabiner1990tut}{{21}{1990}{{Rabiner}}{{}}}
\bibcite{radford2019language}{{22}{2019}{{Radford et~al.}}{{Radford, Wu, Child, Luan, Amodei, and Sutskever}}}
\bibcite{tran2016hmm}{{23}{2016}{{Tran et~al.}}{{Tran, Bisk, Vaswani, Marcu, and Knight}}}
\bibcite{vogel1996hmm}{{24}{1996}{{Vogel et~al.}}{{Vogel, Ney, and Tillmann}}}
\bibcite{wiseman2018hsmm}{{25}{2018}{{Wiseman et~al.}}{{Wiseman, Shieber, and Rush}}}
\bibcite{zaremba2014lstm}{{26}{2014}{{Zaremba et~al.}}{{Zaremba, Sutskever, and Vinyals}}}
\newlabel{sec:hyperparams}{{A}{9}{Hyperparameters}{appendix.A}{}}
\newlabel{sec:supplemental}{{B}{9}{Supplemental Material}{appendix.B}{}}
