\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{dedieu2019learning}
\citation{brown1992}
\newlabel{param}{{2}{1}{Hidden Markov Models}{equation.2.2}{}}
\newlabel{eqn:sparse_emission}{{3}{1}{Sparse Emission HMMs}{equation.3.3}{}}
\newlabel{eqn:sparse_marginalization}{{4}{1}{Sparse Emission HMMs}{equation.3.4}{}}
\citation{ptb}
\citation{wikitext}
\citation{adamw}
\newlabel{eqn:state_dropout}{{5}{2}{State Dropout}{equation.3.5}{}}
\newlabel{eqn:unstructured_dropout}{{6}{2}{State Dropout}{equation.3.6}{}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{tbl:ppl-ptb}{{1}{3}{Perplexities on the \texttt {Penn Treebank} dataset. The FF model is a 256-dim 2-layer feedforward neural network with a window of 4 previous tokens with 0.3 dropout. The LSTM is a 256-dim 2-layer recurrent neural network with 0.3 dropout. The HMM is a 32k-state HMM with 0.5 state dropout. \relax }{table.caption.3}{}}
\newlabel{tbl:ppl-wikitext2}{{2}{3}{Perplexities on the \texttt {wikitext2} dataset. The FF model is a 256-dim 2-layer feedforward neural network with a window of 4 previous tokens with 0.3 dropout. The LSTM is a 256-dim 2-layer recurrent neural network with 0.3 dropout. The HMM is a 32k-state HMM with 0.5 state dropout. \relax }{table.caption.4}{}}
\newlabel{tbl:ppl-assn-ablation}{{3}{3}{The perplexities for the different emission sparsity constraints in a 1024 state HMM as well as larger HMMs for which exact inference without sparsity is too expensive. The quantities $|\mcZ |$ and $k$ are the number of hidden states and the number of states per word respectively. The HMMs with 1024 states do not have any dropout, while the 8k and 16k state HMMs have unstructured dropout. \relax }{table.caption.6}{}}
\newlabel{tbl:ppl-spw-ablation}{{4}{3}{The perplexities for HMMs with 16k states and different numbers of Brown clusters for constraining the emission distribution of the HMMs. $|\mcZ |$ is the total number of hidden states. All models have 0.5 state dropout. \relax }{table.caption.9}{}}
\bibstyle{acl_natbib}
\bibdata{anthology,emnlp2020}
\bibcite{brown1992}{{1}{1992}{{Brown et~al.}}{{Brown, deSouza, Mercer, Pietra, and Lai}}}
\bibcite{dedieu2019learning}{{2}{2019}{{Dedieu et~al.}}{{Dedieu, Gothoskar, Swingle, Lehrach, L\IeC {\'a}zaro-Gredilla, and George}}}
\bibcite{adamw}{{3}{2017}{{Loshchilov and Hutter}}{{}}}
\bibcite{ptb}{{4}{1993}{{Marcus et~al.}}{{Marcus, Santorini, and Marcinkiewicz}}}
\bibcite{wikitext}{{5}{2016}{{Merity et~al.}}{{Merity, Xiong, Bradbury, and Socher}}}
\newlabel{sec:hyperparams}{{A}{4}{Hyperparameters}{appendix.A}{}}
\newlabel{eqn:evidence}{{7}{4}{Gradient of the evidence}{equation.B.7}{}}
\newlabel{eqn:lse_derivative}{{8}{4}{Gradient of the evidence}{equation.B.8}{}}
\newlabel{eqn:plus_derivative}{{9}{4}{Gradient of the evidence}{equation.B.9}{}}
\newlabel{sec:supplemental}{{C}{5}{Supplemental Material}{appendix.C}{}}
