\documentclass[12pt]{article}

\usepackage{mystyle}

\title{Hidden Markov Models}
\author{
Justin T. Chiu
}
\date{\today}


\begin{document}
\maketitle

\begin{abstract}
TODO
\end{abstract}

\section{Problem Setup}
We apply hidden markov models (HMMs) to language modeling,
where we would like to model sentences $x_{1:T}$.
The generative process of an HMM is as follows:
\begin{enumerate}
\item Choose an initial state $z_0 \sim \Cat()$
\item For each time $t \in \set{1, \ldots, T}$ choose a state
$z_t \mid z_{t-1} \sim \Cat()$
\item For each time $t \in \set{0, \ldots, T}$ choose a word
$x_t \mid z_t \sim \Cat()$.
\end{enumerate}

This gives the following joint distribution:
\begin{align*}
\log p_\theta(x_{0:T}, z_{0:T}) &= \log p_\theta(x_0, z_0) + \sum_{t=1}^T \log p_\theta(x_t, z_t \mid z_{t-1})\\
\end{align*}

\section{Parameter estimation}
We maximize the evidence of the observed sentences $\log p(x_{0:T} = \log \sum_{z_{0:T}} p(x_{1:T}, z_{0:T})$.

\subsection{Gradient of evidence}
Let $\psi_0(z_0, z_1) = \log p(x_{0:1}, z_{0:1})$ and $\psi_t(z_{t}, z_{t+1}) = \log p(x_{t+1}, z_{t+1} \mid z_{t})$ for $t \in \set{1, \ldots, T-1}$.
After conditioning on the observed $x_{0:T}$, we can express the evidence as the following:
$Z_x = \log p(x_{0:T}) = \sum_{t=0}^{T-1} \psi_t(z_t, z_{t+1})$
where $Z_x$ is the clamped partition function.

\subsection{}


\subsubsection{Very high training loss}
Surrogate loss is a loose bound, but that is ok.
We proved gradient estimator is correct.


Consider the following example:
Let our KB consist of information about a basketball game,
$$
k = \left\{\begin{array}{c}
    (\textrm{John Doe}, \textrm{POINTS}), \\
    (\textrm{John Doe}, \textrm{REBOUNDS}), \\
    (\textrm{John Doe}, \textrm{FIRST\_NAME}),\\
    (\textrm{John Doe}, \textrm{LAST\_NAME}), \\
    \vdots
\end{array}\right\},
v = \left\{\begin{array}{c}
8,\\
12,\\
\texttt{`John'},\\
\texttt{`Doe'},\\
\vdots 
\end{array}\right\}
$$
aligned to the summary $x = \texttt{`John Doe scored eight points'}$.
We identify the value mention
\texttt{`eight'} as the mention of the record $(k_1, v_1)$.
The record $(k_2,v_2)$ is not mentioned in the text.

\subsection{Model}
We choose our definition of a rationale:
A value for a specific key must be accompanied by the position
of its mention in the text.
Additionally, a value not supported by a value mention in the text
must also be made explicit.

To satisfy this definition of a rationale, we model the values in the KB
with an extract-then-aggregate process.
Our model first makes extractions for each word, then aggregates the word-level choices
in order to resolve possible conflicts.

For every word in the text, the word-level extraction model chooses whether that word is a value mention,
what key the value mention aligns to, and finally what value the mention corresponds to.
We introduce the following latent variables in the extraction model:
\begin{itemize}
\item $m = m_1, \ldots, m_I$ where each $m_i \in \set{0,1}$
    indicates whether word $x_i$ is part of a value mention.
    We aim to identify at least one word in a multiword mention.
\item $a = a_1, \ldots, a_I$ where each
    $a_i \in \set{1, \ldots, J} \cup \set{\texttt{None}}$
    indicates that word $x_i$ mentions the record $(k_{a_i}, v_{a_i})$.
\item $z = z_1, \ldots, z_I$ where each $z_i \in \mcV\cup \set{\texttt{None}}$
    translates the value mention containing word $x_i$
    into the canonical representation of
    the value as determined by the schema of the KB.
    For example, the KB may only store numbers in numerical form,
    as opposed to alphabetical.
\end{itemize}

Our extract-then-aggregate model specifies the joint distribution over the values $v$ and latent variables $z,a,m$
given the text $x$ and keys $k$:
\begin{equation}
\label{eqn:prob}
\begin{aligned}
&p(v,z,a,m\mid x,k)\\
%&= p(v\mid z,a,m,x,k)p(z\mid x)p(a\mid x,k)p(m\mid x)\\
&= p(v\mid z,a,m,x,k)p(z,a,m\mid x,k)\\
&= \left(\prod_{j=1}^J \underbrace{p(v_j\mid z,a,m,x,k)}_{\textrm{Aggregation}}\right)
    \underbrace{\left(\prod_{i=1}^I
        \underbrace{p(z_i\mid m_i,x)}_{\textrm{Translation}}
        \underbrace{p(a_i\mid m_i,x,k)}_{\textrm{Alignment}}
        \underbrace{p(m_i\mid x)}_{\textrm{Identification}}
    \right)}_{\textrm{Extraction}}
\end{aligned}
\end{equation}

The extraction model has three components.
The model must identify whether word $x_i$
is a mention using the \textit{identification} model $p(m_i \mid x)$,
align word $x_i$ to a key $k_{a_i}$ via the \textit{alignment} model $p(a_i \mid m_i, x, k)$,
and translate the word $x_i$ into a value $z_i$ with the
\textit{translation} model $p(z_i \mid m_i, x)$.

This model assumes that each word $x_i$ can explain at most one record in the KB.
Additionally, we make the simplifying assumption
that a value mention's translation is unambiguous and independent from the alignment to a key.

It is possible for there to be conflicts in the word-level extraction model if 
two words are mentions with the same alignment but different values.
The aggregation model $p(v_j \mid z,a,m,x,k)$ is necessary to resolve these conflicts,
and uses the per-word latent variables $z_i,a_i,m_i$ to predict the values of the KB.

We give the parameterizations of all conditional distributions in the following section.

\begin{figure}[h]
\begin{center}
\resizebox {.35\columnwidth} {!} {
\begin{tikzpicture}
\node[obs] (Y) {$x$};
\node[latent, below=of Y](Ai) {$a_i$};
\node[latent, left=of Ai](Zi) {$z_i$};
\node[latent, right=of Ai](Mi) {$m_i$};

%\node[obs, below=of A] (D) {$k_j$};
%\node[obs, below=of Vi] (V) {$v_j$};
\node[obs, below=of Ai] (K) {$k$};
\node[obs, below=of Zi] (V) {$v$};

\plate {i} {(Zi)(Ai)(Mi)} {$I$};
%\plate  {j} {(D)(V)} {$J$};

\draw[->] (Y) -- (Mi);
\draw[->] (Y) -- (Ai);
\draw[->] (Y) -- (Zi);
%\draw[->] (A) -- (Vi);
\draw[->] (Mi) to [bend left = 25] (Zi);
\draw[->] (Mi) to [bend left = 25] (Ai);
\draw[->] (K) -- (Ai);
%\draw[-] (Vi) -- (V);
%\draw[-] (A) -- (V);
%\draw[-] (C) -- (V);
%\draw[-] (E) to [bend right = 25] (V);
%\draw[-] (T) -- (V);
\draw[->] (Zi) -- (V);
\draw[->] (Ai) -- (V);
\draw[->] (Mi) -- (V);
\draw[->] (K) -- (V);

\end{tikzpicture}
} %% end resize
\end{center}
\caption{Our model predicts word-level values and alignments
then aggregates those choices over all indices $i$ to
make a single decision for each value.
Each word has the following latent variables:
the mention $m_i \in \set{0,1}$ indicates whether word $x_i$ is a value mention,
the alignment $a_i$ gives the key $k_{a_i}$ that $x_i$ aligns to,
and the value $z_i$ gives the canonical value that $x_i$ translates to.
}
\label{fig:infmodel}
\end{figure}

\subsection{Parameterization}
Let $\bh_i \in \R^d$ be a contextual embedding of the word $x_i$,
and $E$ an embedding function that maps keys $k$ to vectors in $\R^{d}$.
\begin{enumerate}
\item Identification: We use the contextual embedding to  predict
whether a word $x_i$ is part of a value mention.
\begin{equation}
p(m_i \mid x) \propto \exp(W_m\bh_i)_{m_i},
\end{equation}
with $W_m \in \R^{2 \times d}$.
\item Alignment: We use a bilinear function of the cell embeddings and
contextual embeddings to parameterize the alignment distribution.
We align $a_i = \texttt{None}$ if a word is not a mention.
\begin{equation}
\begin{aligned}
p(a_i \mid m_i=1,x,k) &\propto \exp(E(k_{a_i})^T W_a \bh_i)\\
p(a_i=\texttt{None} \mid m_i=0,x,k) &= 1
\end{aligned}
\end{equation}
with $W_a \in \R^{d \times d}$.
\item Translation: We use the contextual embedding to translate a word
into a value. 
If a word is chosen not to be a mention such that $m_i=0$,
we set $z_i = \texttt{None}$.
\begin{equation}
\begin{aligned}
p(z_i \mid m_i=1, x) &\propto \exp(W_z \bh_i)_{z_i}\\
p(z_i = \texttt{None} \mid m_i=0, x) &= 1
\end{aligned}
\end{equation}
with $W_z \in \R^{|\mcV| \times d}$.
\item Aggregation:
We choose the value associated with a particular key in the KB
by first using the translated values of any mentions aligned to the
corresponding key.
If no mention is aligned to the key, we choose the value given only the key.

Formally, the conditional distribution is given by
\begin{align}
p(v_j \mid z,a,m,k) &\propto \begin{cases}
    \prod_{i}
        \exp(\psi(v_j, z_i,a_i,m_i)),  & \exists i, m_i = 1 \wedge a_i = j\\
    %p(v_j \mid e_j,t_j), & \textrm{otherwise}
    \exp(E(v_j)^TW_v [E(k_j)]), & \textrm{otherwise}
\end{cases}\\
\psi(v_j, z_i, a_i, m_i) &= \mathbf{1}(v_j = z_i, a_i = j, m_i=1)%\\
%p(v_j \mid e,t) & \propto \exp(E(v_j)^TW_v [E(e),E(t)])
\end{align}
with $W_v \in \R^{d\times d}$.
\end{enumerate}

\section{Training and Inference}
We would like to optimize the marginal likelihood of the values $v$ given the 
text $x$ and keys $k$:
\begin{equation}
\log p(v \mid x,k) = \log \sum_{z,a,m} p(v,z,a,m \mid x,k)
\end{equation}
However, maximizing $\log p(v \mid x,k)$ directly is very expensive 
as the summation over variables $z,a,m$ is intractable;
in particular,
the summation has computational complexity $O((|\mcV|\cdot J)^{I})$.
We therefore resort to approximate inference,
specifically amortized variational inference.

\subsection{Inference Network}
We introduce an inference network $q(z,a,m\mid v,x,k)$
and optimize the following lower bound on the marginal likelihood
with respect to the parameters of both $p$ and $q$:
\begin{equation}
\label{eqn:lowerbound}
\log p(v\mid x,k) \geq
\Es{q(z,a,m\mid v,x,k)}{\log \frac{p(v,z,a,m\mid x,k)}{q(z,a,m\mid v,x,k)}}
\end{equation}

The joint distribution of $q(z,a,m\mid v,x,k)$ in the inference network is given by
\begin{equation}
\label{eqn:elbo}
\begin{aligned}
q(z,a,m\mid v,x,k) &= \prod_i
    \underbrace{q(z_i \mid a,v,x)}_{\textrm{Translation}}
    \underbrace{q(a_i \mid v,x,k)}_{\textrm{Alignment}}
    \underbrace{q(m_i \mid v,x)}_{\textrm{Identification}}
\end{aligned}
\end{equation}

In the inference network,
we use the contextual embedding $\bh_i \in \R^d$ of the word $x_i$
and extend the embedding function $E$ to embed not only keys but also values in $\R^d$.
We introduce the following attention weights over records
in order to get a weighted representation of the KB for each word $x_i$:
\begin{equation}
\label{eqn:attn}
\begin{aligned}
\bg_j &= [E(k_j); E(v_j)]\\
\alpha_{ij} &\propto \exp(\bg_{j}^T W_\alpha \bh_i)
\end{aligned}
\end{equation}
with $W_\alpha \in \R^{2d \times d}$.

We propose to parameterize the inference network $q(z,a,m\mid v,x,k)$ as follows:
\begin{enumerate}
\item Identification: To predict whether $x_i$ is a mention,
    the inference network uses the weighted representation of the KB $\alpha$
    along with the contextual representation $\bh_i$
    \begin{equation}
    p(m_i \mid v,x) \propto
    \exp\left(V_m
    \textrm{MLP}\left(\left[\sum_j \alpha_{ij} \cdot \bg_j; \bh_i\right]\right)\right)_{m_i}
    \end{equation}
    where MLP is a neural network and $V_m \in \R^{2 \times d}$.
\item Alignment: The alignment model uses the attention weights in Eqn.~\ref{eqn:attn}:
    $q(a_i = j \mid v,x,k) = \alpha_{ij}$.
\item Translation: The translation model
    $q(z_i \mid a_i,v,x) = \mathbf{1}(z_i = v_{a_i})$
    conditions on the alignment $a_i$ and ensures the translated value $z_i$ is consistent
    with the alignment. 
\end{enumerate}

We optimize the objective in Eqn.~\ref{eqn:elbo}
with respect to both the extraction model $p$ and the inference network $q$
via gradient ascent using the score function gradient estimator.
We utilize a leave-one-out baseline for variance reduction.

(Under consideration, could hopefully remove pretraining by parameterizing translation
model using local features such as embedding or conv, but previous
experiments on this front were negative)
As our model is highly unidentifiable,
we bias our extraction model by pretraining $p(z_i \mid x)$ on lexical match.

\section{Evaluation}
We evaluate whether the model can discover
and locate the subset of records in a KB that are expressed in the text
without observing the values of the KB.
We compare the extractions of our model with a ground truth set of records,
reporting the micro-averaged precision, recall, and F1 score.
%We also consider as well as the location of their value mention in text.

We perform extraction by finding
$${\arg\max}_{z,a,m} p(z,a,m\mid x,k).$$
We obtain the record $(k_j, v_j)$ at word $x_i$ if $m_i=1$,
$a_i = j$ and $z_i = v_j$.
% Can also use approximate posterior
%The extraction is considered correct if the ground truth
%contains a fact with the same entity, type, value, and location.
%Alternatively, we could condition on an existing database
%As we assumed a fully factored model over $z,a,m$ 

\begin{comment}

\subsection{Approximate the posterior of a generative model}
Alternatively, we may introduce a generative model of text $q(x, v)$ 
that inverts the relation extraction model $p(v \mid x)$ and optimize the following objective:
\begin{equation}
\log q(x,v) - KL[p(v,z,a,m \mid x) || q(v,z,a,m \mid x)]
\end{equation}
which entails training the generative model of text while approximating its
posterior with the information extraction system.

decompose the training of our extraction system $p(v\mid x)$ into two stages:
In the first stage we train $p(z,a,m\mid x,d)$ to approximate the posterior
of a conditional model of text given a complete KB $q(x,z,a,m \mid v,d)$.
This has the benefit of allowing us to exert control over where value mentions are detected
through our design of the text model $q$.

In the second stage, we have two choices:
a) train $p(v \mid z,a,m,x,d)$ to approximate the posterior of a full generative model of text and
the values of KB $q(x,v\mid d)$.
b) train $p(v \mid z,a,m,x,d)$ using the following lower bound:
\begin{equation}
\label{eqn:lowerbound2}
\log p(v\mid x,k) \geq
\Es{p(z,a,m\mid x,d)}{\log p(v\mid z,a,m, x,d)}
\end{equation}
Ideally the bound in Eqn.~\ref{eqn:lowerbound2}
should not be looser than the one presented in Eqn.~\ref{eqn:lowerbound},
as conditioning on the observed values of a KB should not reduce the entropy of
a good alignment model.

How do we split the gradient over time?

\end{comment}

%\bibliography{bib}
%\bibliographystyle{acl_natbib}

\end{document}
