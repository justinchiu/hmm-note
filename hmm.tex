\documentclass[12pt]{article}

\usepackage{mystyle}

\title{Hidden Markov Models}
\author{
Justin T. Chiu
}
\date{\today}


\begin{document}
\maketitle

\begin{abstract}
TODO
\end{abstract}

\section{Introduction}
TODO

\section{Problem Setup}
We focus on language modeling,
where we learn a distribution over sequences of tokens
at the word level $\bx = \set{x_0, \ldots, x_T}$, with each token $x_t$
an element of the finite vocabulary $\mcX$.

\subsection{Hidden Markov Models}
Hidden Markov Models (HMMs) specify a joint distribution over 
the observed words $\bx$ and discrete latent states $\bz = \set{z_0, \ldots, z_T}$,
each $z_t$ from finite set $\mcZ$,
with the following generative process:
For every time step $t \in \set{0,\ldots,T}$, choose a state given the previous state
$z_t \mid z_{t-1}$ from the transition distribution $p(z_t \mid z_{t-1})$,
where the first state $z_0$ is chosen from the starting distribution $p(z_0)$.
Then choose a word given the current state $x_t \mid z_t$ from the emission distribution $p(x_t \mid z_t)$.
This yields the joint distribution
\begin{equation}
p(\bx, \bz)
= p(z_0)p(x_0 \mid z_0)\prod_{t=1}^T p(x_t\mid z_t)p(z_t \mid z_{t-1})
\end{equation}

%By virtue of modeling the joint distribution over $\bx_{0:T},\bz_{0:T}$,
%HMMs maintain uncertainty over both observed words the unobserved latent states.
%Contrast this with another class of language models, recurrent neural networks (RNNs),
%which only maintain uncertainty over the observed $\bx_{0:T}$.

%At every timestep $t$, the generative process bottlenecks all information from the past
%through a single discrete state $z_t$ from a finite set.
%This contrasts 

The conditional distributions are parameterized as follows
\begin{equation}
\label{param}
\begin{aligned}
p(z_0) &\propto \phi_{z_0}\\
p(z_t \mid z_{t-1}) &\propto \phi_{z_tz_{t-1}}\\
p(x_t \mid z_t) &\propto \phi_{x_tz_t}
\end{aligned}
\end{equation}
where each $\phi$ may be parameterized by a scalar or a neural network for parameter sharing.

\section{Model}
For language modeling, we are interested in the distribution over the observed words
obtained by marginalizing over the latent states $p(\bx) = \sum_{\bz}p(\bx,\bz)$.
This sum can be computed in time $O(T|\mcZ|^2)$ via dynamic programming,
which becomes prohibitive if the number of latent states $|\mcZ|$ is large.
However, in order to increase the capacity of HMMs, we must consider large $\mcZ$.
We outline several techniques to manage the computational complexity of marginalization in HMMs.  

\subsection{Sparse Emission HMMs}
We introduce a sparse emission constraint that allows us to
efficiently perform conditional inference in large state spaces.
In a manner inspired by \citet{dedieu2019learning},
we constrain each word $x$ to be emit only by $z \in \mcC_x \subset \mcZ$:
\begin{equation}
\label{eqn:sparse_emission}
p(x \mid z) \propto \begin{cases}
\phi_{zx} & z \in \mcC_x \\
0 & \textrm{otherwise}
\end{cases}
\end{equation}
This allows us to perform marginalization as follows
\begin{equation}
\label{eqn:sparse_marginalization}
\begin{aligned}
p(\bx) &= \sum_{z_0} p(z_0)p(x_0 \mid z_0)
    \sum_{z_1}p(z_1\mid z_0)p(x_1 \mid z_1)\cdots
    \sum_{z_T}p(z_T \mid z_{T-1})p(x_T \mid z_T)\\
&= \sum_{z_0 \in \mcC_{x_0}} p(z_0)p(x_0 \mid z_0)
    \sum_{z_1 \in \mcC_{x_1}} p(z_1\mid z_0)p(x_1 \mid z_1)\cdots
    \sum_{z_T \in \mcC_{x_T}} p(z_T \mid z_{T-1})p(x_T \mid z_T)
\end{aligned}
\end{equation}
which takes $O(T \max_x(|\mcC_x|)^2)$ computation.

We experiment with two constraint sets, both of which are specified 
as hyperparameters and remain fixed for a given model.
The first constraint set is obtained by sampling subsets of
states for each word i.i.d. from a uniform distribution over subsets.
The second constraint set uses Brown Clusters \citep{brown1992}
to assign all words in a given Brown cluster the same subset of states.
This constraint set is advantageous, as it admits further optimizations
that we discuss below.

\subsection{State Dropout}
We introduce a variant of dropout called state dropout that operates on the
start and transition distributions.
The goal of state dropout is to reduce the complexity of marginalization in a manner
identical to the sparse emission constraints.

State dropout samples a mask over state partitions
$\mathbf{b}_\mcC \in \set{0,1}^{|\mcC|}$ for each partition $\mcC \subset \mcZ$,
such that $|\mathbf{b}_\mcC| = k$.
Since this procedure is straightforward if the $\mcC_x$ are disjoint partitions,
we only consider its application to the Brown constraint set.

The start and transition distributions with dropout are given by
\begin{equation}
\label{eqn:state_dropout}
\begin{aligned}
p(z_0) &\propto \begin{cases}
\phi_{z_0} & \mathbf{b}_{\mcC_{x_0} z} = 1\\
0 & \textrm{otherwise}\\
\end{cases}
p(x \mid z) &\propto \begin{cases}
\phi_{zx} & z \in \mcC_x \wedge \mathbf{b}_{\mcC_x z} = 1\\
0 & \textrm{otherwise}
\end{cases}
\end{aligned}
\end{equation}


\subsection{Parameterization}


\section{Experiments}
\subsection{Language Modeling}
\subsubsection{Datasets}
\paragraph{Penn Treebank}
\paragraph{WikiText-2}

\subsubsection{Results}


\subsection{POS Induction}

\appendix
\section{Parameter estimation}
We maximize the evidence of the observed sentences
$\log p(\bx_{0:T}) = \log \sum_{\bz_{0:T}} p(\bx_{1:T}, \bz_{0:T})$
via gradient ascent.

\subsection{Gradient of the evidence}
Let $\psi_0(z_0, z_1) = \log p(x_0, z_0)$ and
$\psi_t(z_{t}, z_{t+1}) = \log p(x_{t+1}, z_{t+1} \mid z_{t})$ for $t \in \set{1, \ldots, T-1}$.
Additionally, let $\oplus$ and $\otimes$ be addition and multiplication in the log semiring.
After conditioning on the observed $\bx_{0:T}$, we can express the evidence as the following:
\begin{equation}
\label{eqn:evidence}
A_x = \log p(\bx_{0:T}) = \bigoplus_{\bz_{0:T}}\bigotimes_{t=0}^{T-1} \psi_t(z_t, z_{t+1})
\end{equation}
where $A_\bx$ is the clamped log partition function (after observing $\bx_{0:T}$).

We show that the gradient of the log partition function with respect to the $\psi_t(z_t, z_{t+1})$
is the first moment (a general result for the cumulant generating function
of exponential family distributions). Given the value of the derivative
$\frac{\partial A_\bx}{\partial \psi_t(z_t, z_{t+1})}$,
we can then apply the chain rule to compute the total derivative with respect to
downstream parameters. For example, the gradient with respect to the transition matrix
of the HMM is given by
$$\frac{\partial A_\bx}{\partial \theta_{ij}}
= \sum_t \frac{\partial A_\bx}{\partial \psi_t(i,j)}
\frac{\partial \psi_t(i,j)}{\partial \theta_{ij}}$$

Recall that the derivative of logaddexp ($\oplus$ in the log semiring) is
\begin{equation}
\label{eqn:lse_derivative}
\frac{\partial}{\partial x} x \oplus y
= \frac{\partial}{\partial x} \log e^x + e^ y
= \frac{e^x}{e^x + e^y}
= \exp(x - (x \oplus y))
\end{equation}
while the derivative of addition ($\otimes$ in the log semiring) is
\begin{equation}
\label{eqn:plus_derivative}
\frac{\partial}{\partial x} x \otimes y = 1
\end{equation}

The derivative of the clamped log partition function $A_\bx$ is given by
\begin{align*}
\frac{\partial A_\bx}{\partial \psi_t(i,j)}
&= \frac{\partial}{\partial \psi_t(i,j)} \bigoplus_{\bz_{0:T}}
    \bigotimes_{t'} \psi_{t'}(z_{t'}, z_{t'+1})\\
&= \frac{\partial}{\partial \psi_t(i,j)} \left(
        \left(\bigoplus_{\bz_{0:T}:z_t = i, z_{t+1} = j}
        \bigotimes_{t'} \psi_{t'}(z_{t'}, z_{t'+1})\right)
        \bigoplus
        \left(\bigoplus_{\bz_{0:T}:z_t \ne i, z_{t+1} \ne j}
        \bigotimes_{t'} \psi_{t'}(z_{t'}, z_{t'+1})\right)
    \right)\\
&= \exp\Bigg(\left(\bigoplus_{\bz_{0:T}:z_t = i, z_{t+1} = j} 
    \bigotimes_{t'} \psi_{t'}(z_{t'}, z_{t'+1})\right)
    \\
&\qquad - \left(\bigoplus_{\bz_{0:T}:z_t = i, z_{t+1} = j}
        \bigotimes_{t'} \psi_{t'}(z_{t'}, z_{t'+1})\right)
        \bigoplus
        \left(\bigoplus_{\bz_{0:T}:z_t \ne i, z_{t+1} \ne j}
        \bigotimes_{t'} \psi_{t'}(z_{t'}, z_{t'+1})\right)
    \Bigg)\\
&= \exp\left(\left(\bigoplus_{\bz_{0:T}:z_t = i, z_{t+1} = j} 
    \bigotimes_{t'} \psi_{t'}(z_{t'}, z_{t'+1})\right)
    - A_\bx\right)
\end{align*} 
which is the value at $i$ and $j$ of the
edge marginal for $z_t, z_{t+1}$ obtained via the forward-backward algorithm.
The second equality is a result of the associativity of $\oplus$;
the third equality is a result of Eqn. \ref{eqn:lse_derivative};
and the fourth equality from Eqn. \ref{eqn:plus_derivative}.

%\subsubsection{Very high training loss}
%It is okay if the surrogate loss is a loose bound.
%We proved gradient estimator is correct.
%This just means that the ELBO under a pairwise approximation is loose.

\section{Dropout}
\begin{itemize}
\item Total number of states $K$
\item $M$ Partitions (disjoint subsets) $C_l \subset [K]$ of states, $l \in [L]$
\item Unnormalized transition matrix $T \in \mathbb{R}^{K \times K}$,
    where $T_{i,j}$ gives the probability of transitioning from state $i$ to $j$
\item Unnormalized starting vector $s \in \mathbb{R}^K$
\item Transition matrix mask $B \in \set{0,1}^{K \times K}$
\item Starting vector mask $c \in \set{0, 1}^K$
\end{itemize}

We sample masks $B,c$ then normalize the elementwise products $B \odot T$
and $c \odot s$ to obtain the transition and start probabilities.
We now outline the strategies for producing dropout masks.

\subsection*{Transition Dropout}
We sample a dropout mask iid for each entry of the transition matrix.
\begin{itemize}
\item Sample $B_{ij} \overset{iid}{\sim} \textrm{Bern}(0.5), \forall i,j \in [K]$
\end{itemize}

Advantage: We can compute the transition matrix using a dense subset of
the matrix, but may have a decent amount of padding since the number of
nonzero columns for a given row may differ.
\subsection*{Column dropout}
We sample a dropout mask for the columns of the transition matrix,
zeroing out an entire column.
\begin{itemize}
\item Sample $m \in\{0,1\}^K, m_j \overset{iid}{\sim} \textrm{Bern}(0.5)$
\item Set $B_{i j} = m_j, \forall i,j \in [K]$
\end{itemize}

Advantage: Eliminates column padding during transition matrix computation.

\subsection*{Start + column dropout}
We sample a dropout mask for the columns of the matrix as well as the elements of the start vector independently.
\begin{itemize}
\item Sample $m \in\{0,1\}^K, m_j \overset{iid}{\sim} \textrm{Bern}(0.5)$
\item Sample $m' \in\{0,1\}^K, m_i' \overset{iid}{\sim} \textrm{Bern}(0.5)$
\item Set $B_{i j} = m_j, \forall i,j \in [K]$
\item Set $c_{i} = m'_i, \forall i \in [K]$
\end{itemize}

\subsection*{State dropout}
We sample a dropout mask for all the states, and apply the same mask
to the columns of the transition matrix as well as the start vector.
\begin{itemize}
\item Sample $m \in\{0,1\}^K, m_j \overset{iid}{\sim} \textrm{Bern}(0.5)$
\item Set $B_{ij} = m_j, \forall i,j \in [K]$
\item Set $c_{i} = m_i, \forall i \in [K]$
\end{itemize}

Advantage: Allows us to decrease the number of rows we must compute
for the transition matrix as well.

\subsection*{State subset dropout}
We sample a dropout mask over state such that we have $n$ 1's.
\begin{itemize}
\item Sample $m \in \{0,1\}^k$ such that $|m| = n$
\item Set $B_{ij} = m_j, \forall i,j \in [K]$
\item Set $c_{i} = m_i, \forall i \in [K]$
\end{itemize}

Advantage: Eliminates variance in the size of the transition matrix.

\subsection*{Partition subset dropout}
Given a set of partitions $\{C_l\}$ of equal size $r = K/L$,
we sample a dropout mask of size $n$ for each partition.
\begin{itemize}
\item Sample each $M \in \{0,1\}^{L \times r}, |M_l| = n$ for $l \in [L]$
\item Set $B_{ij} = \sum_l 1(j \in C_l) M_{l,j \% r}, \forall i,j \in [K]$
\end{itemize}

Advantage: Eliminates variance in the number of dropped states assigned
to a cluster.

\section{Variational Dropout}
TODO: is an efficient baseline possible?

\section{Factorial HMM}
We implement the model and inference procedure described in
\citet{nepal2014fhmm,ghahramani1997fhmm} for factorial hidden markov models (FHMMs)
with discrete emissions.

\bibliographystyle{plainnat}
\bibliography{bib}

\end{document}
